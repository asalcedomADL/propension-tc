{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import boto3.session\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(s3_path):\n",
    "    \"\"\"\n",
    "    Autor: ADL\n",
    "    Email: ADL\n",
    "    Descripcion; Función para ver los elementos que hay en una ruta determinada.\n",
    "    Parámetros: texto con la ruta de interés\n",
    "    Retorno: Listado de los elementos que se encuentra en la ruta determinada.\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3_path.split('//')[1].split('/')[0]\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    prefix = '/'.join(s3_path.split('//')[1].split('/')[1:])\n",
    "    n_sub = len(prefix.split('/'))\n",
    "    list_obj = []\n",
    "    for object_summary in my_bucket.objects.filter(Prefix=prefix):\n",
    "        obj_x = object_summary.key.split('/')[n_sub - 1]\n",
    "        if not obj_x in list_obj:\n",
    "            list_obj.append(obj_x)\n",
    "    return(list_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consolidado',\n",
       " 'plenas-individuales',\n",
       " 'plenas-separados',\n",
       " 'pymes-individuales',\n",
       " 'pymes-separados']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_files('s3://data-bpop-dev-sandbox/estandarizado/productos/libranzas/')\n",
    "# list_files('s3://data-bpop-dev-sandbox/estandarizado/productos/activo/')\n",
    "list_files('s3://data-commons-dev-sandbox-de/activos_propios/fuentes_externas/supersociedades/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths_activos = { 'libranzas': ['s3://data-bpop-dev-sandbox/estandarizado/productos/libranzas/productos_libranzas_dwh_M202106'],  ## Mensual 1 mes atras respecto al mes que esta corriendo - coger v2\n",
    "         'clientes': ['s3://data-bpop-dev-sandbox/estandarizado/clientes/identificacion/clientes_identificacion_mdm_D20210630'],  ## Mensual 1 mes atras - coger v2 #### NO SE NECESITA QUITAR\n",
    "\n",
    "         'activo': ['s3://data-bpop-dev-sandbox/estandarizado/productos/activo/productos_activo_sarc_M202105'],  ## Mensual 2 meses atras - coger v2\n",
    "\n",
    "         'canpañas': 's3://data-bpop-dev-refined/gestion-campanas/preaprobado-tc/',\n",
    "         'crm': ['s3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20200228-20200601.csv',  ## Quemado\n",
    "                 's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20190801-20200201.csv',  ## Quemado\n",
    "                 's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20190104-20190702.csv'],  ## Quemado\n",
    "         'vendors':['s3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-vend/campanas-tc-nuevas-proc-vendors_H20200401-20200601.csv',  ## Quemado\n",
    "                    's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-vend/campanas-tc-nuevas-proc-vendors_H20190101-20200301.csv'],  ## Quemado\n",
    "\n",
    "         'activaciones': ['s3://adl-refined-dev-popular/data_orig/productos/activo-tarjeta-credito-activadas/productos_activos-tarjeta-credito-activadas_H202003-202005.csv',  ## Quemad o\n",
    "                          's3://adl-refined-dev-popular/data_orig/productos/activo-tarjeta-credito-activadas/productos_activos-tarjeta-credito-activadas_H20180101-20200327.csv'],  ## Quemado\n",
    "         'acti_diarias': ['s3://data-bpop-dev-sandbox/estandarizado/productos/activo-tarjeta-credito-nueva'],  ##TodosDiarios\n",
    "\n",
    "         'buro': 's3://data-bpop-dev-sandbox/estandarizado/riesgos-creditos/buro-comportamiento-financiero',  ##TodosDiarios\n",
    "\n",
    "         'pasivo_cc': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-cc/productos_pasivo-sfb-cc_aaatr_M',\n",
    "\n",
    "         'pasivo_cdt': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-cdts/productos_pasivo-sfb-cdts_aaatr_M',\n",
    "                       \n",
    "         'pasivo_ca': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-ca/productos_pasivo-sfb-ca_aaatr_M'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation(input_paths_activos):\n",
    "    \n",
    "    libranzas_df = process_libranzas(input_paths_activos)\n",
    "    print('process_libranzas_successfully')\n",
    "    ids_final2 = processing_filtering(input_paths_activos)\n",
    "    print('processing_filtering_successfully')\n",
    "    data_activos = process_activos(input_paths_activos)\n",
    "    print('process_activos_successfully')\n",
    "    tabla_final = process_camp_activos(input_paths_activos)\n",
    "    print('process_camp_activos_successfully')\n",
    "    buro_mdt = process_buro(input_paths_activos)\n",
    "    print('process_buro_successfully')\n",
    "    pasvivos_df = process_pasivos(input_paths_activos)\n",
    "    print('process_pasivos_successfully')\n",
    "    \n",
    "    poblacion_total = consolidado_premdt(ids_final2)\n",
    "    print('consolidado_premdt_successfully')\n",
    "    \n",
    "    return libranzas_df, ids_final2, data_activos, tabla_final, buro_mdt, pasvivos_df, poblacion_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libranzas_df, ids_final2, data_activos, tabla_final, buro_mdt, pasvivos_df, poblacion_total = preparation(input_paths_activos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_numero_cliente</th>\n",
       "      <th>id_tp_cd</th>\n",
       "      <th>num_lib_solicitadas</th>\n",
       "      <th>prom_monto_novado</th>\n",
       "      <th>prom_n_cuotas</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27079010</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1</td>\n",
       "      <td>16502114.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>202104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14972564</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1</td>\n",
       "      <td>6788899.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>202104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11222544</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1</td>\n",
       "      <td>80338044.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>202104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27076024</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1</td>\n",
       "      <td>17376076.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>202104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19207951</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>202104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_numero_cliente  id_tp_cd  num_lib_solicitadas  prom_monto_novado  \\\n",
       "0           27079010   1000003                    1         16502114.0   \n",
       "1           14972564   1000003                    1          6788899.0   \n",
       "2           11222544   1000003                    1         80338044.0   \n",
       "3           27076024   1000003                    1         17376076.0   \n",
       "4           19207951   1000003                    1                0.0   \n",
       "\n",
       "   prom_n_cuotas  periodo  \n",
       "0          120.0   202104  \n",
       "1           48.0   202104  \n",
       "2           99.0   202104  \n",
       "3           92.0   202104  \n",
       "4          120.0   202104  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#libranzas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cliente</th>\n",
       "      <th>fecha_camp</th>\n",
       "      <th>id_numero_cliente</th>\n",
       "      <th>id_tp_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102652297589965301</td>\n",
       "      <td>202101</td>\n",
       "      <td>27079010</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102652296060855001</td>\n",
       "      <td>202101</td>\n",
       "      <td>14972564</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102652304429247101</td>\n",
       "      <td>202101</td>\n",
       "      <td>80354687</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102652295542372001</td>\n",
       "      <td>202101</td>\n",
       "      <td>11222544</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102652297598623801</td>\n",
       "      <td>202101</td>\n",
       "      <td>27076024</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_cliente  fecha_camp id_numero_cliente id_tp_cd\n",
       "0  102652297589965301      202101          27079010  1000003\n",
       "1  102652296060855001      202101          14972564  1000003\n",
       "2  102652304429247101      202101          80354687  1000003\n",
       "3  102652295542372001      202101          11222544  1000003\n",
       "4  102652297598623801      202101          27076024  1000003"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nroident</th>\n",
       "      <th>tipo_ident_hom</th>\n",
       "      <th>fecha_activos</th>\n",
       "      <th>amortizacion_min_lb</th>\n",
       "      <th>cuota_max_lb_smlv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206</td>\n",
       "      <td>1000003</td>\n",
       "      <td>202104</td>\n",
       "      <td>0.840337</td>\n",
       "      <td>0.531937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1112</td>\n",
       "      <td>1000003</td>\n",
       "      <td>202104</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.312141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272</td>\n",
       "      <td>1000003</td>\n",
       "      <td>202104</td>\n",
       "      <td>0.960803</td>\n",
       "      <td>1.563886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1974</td>\n",
       "      <td>1000003</td>\n",
       "      <td>202104</td>\n",
       "      <td>0.339418</td>\n",
       "      <td>0.835206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2460</td>\n",
       "      <td>1000003</td>\n",
       "      <td>202104</td>\n",
       "      <td>0.512831</td>\n",
       "      <td>1.785160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nroident  tipo_ident_hom  fecha_activos  amortizacion_min_lb  \\\n",
       "0       206         1000003         202104             0.840337   \n",
       "1      1112         1000003         202104             0.417281   \n",
       "2      1272         1000003         202104             0.960803   \n",
       "3      1974         1000003         202104             0.339418   \n",
       "5      2460         1000003         202104             0.512831   \n",
       "\n",
       "   cuota_max_lb_smlv  \n",
       "0           0.531937  \n",
       "1           0.312141  \n",
       "2           1.563886  \n",
       "3           0.835206  \n",
       "5           1.785160  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_activos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cedula</th>\n",
       "      <th>mes_campaña</th>\n",
       "      <th>acep_oferta_prev</th>\n",
       "      <th>mes_ult_act</th>\n",
       "      <th>mes_ult_camp</th>\n",
       "      <th>num_act_utl_meses</th>\n",
       "      <th>num_camp_ult_meses</th>\n",
       "      <th>num_meses_ult_actv</th>\n",
       "      <th>num_meses_ult_camp</th>\n",
       "      <th>num_no_aceptado</th>\n",
       "      <th>data_camp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66850</th>\n",
       "      <td>3709</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60661</th>\n",
       "      <td>3876</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20046</th>\n",
       "      <td>4444</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28159</th>\n",
       "      <td>8205</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27802</th>\n",
       "      <td>15237</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cedula mes_campaña  acep_oferta_prev mes_ult_act mes_ult_camp  \\\n",
       "66850    3709  2019-12-01               NaN         NaT          NaT   \n",
       "60661    3876  2019-11-01               NaN         NaT          NaT   \n",
       "20046    4444  2019-05-01               NaN         NaT          NaT   \n",
       "28159    8205  2019-06-01               NaN         NaT          NaT   \n",
       "27802   15237  2019-06-01               NaN         NaT          NaT   \n",
       "\n",
       "       num_act_utl_meses  num_camp_ult_meses  num_meses_ult_actv  \\\n",
       "66850                NaN                 NaN                 NaN   \n",
       "60661                NaN                 NaN                 NaN   \n",
       "20046                NaN                 NaN                 NaN   \n",
       "28159                NaN                 NaN                 NaN   \n",
       "27802                NaN                 NaN                 NaN   \n",
       "\n",
       "       num_meses_ult_camp  num_no_aceptado  data_camp  \n",
       "66850                 NaN              NaN     201912  \n",
       "60661                 NaN              NaN     201911  \n",
       "20046                 NaN              NaN     201905  \n",
       "28159                 NaN              NaN     201906  \n",
       "27802                 NaN              NaN     201906  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo_id</th>\n",
       "      <th>tipo_id_homologado</th>\n",
       "      <th>no_identificacion</th>\n",
       "      <th>id_cliente</th>\n",
       "      <th>fecha_buro</th>\n",
       "      <th>numero_obligaciones_activasdif</th>\n",
       "      <th>porcentaje_utilizacion</th>\n",
       "      <th>quanto_mod</th>\n",
       "      <th>valor_cuotas_codeudores_smlv</th>\n",
       "      <th>valor_utilizado_smlv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735871</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>206</td>\n",
       "      <td>421952294130855802</td>\n",
       "      <td>202101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3669000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871475</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>357</td>\n",
       "      <td>702552294130987801</td>\n",
       "      <td>202101</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>2.1094</td>\n",
       "      <td>3432000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.973526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631622</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1112</td>\n",
       "      <td>543052294130923801</td>\n",
       "      <td>202101</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1676000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656626</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1272</td>\n",
       "      <td>962752294130988601</td>\n",
       "      <td>202101</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>18188000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.737771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713245</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>1898</td>\n",
       "      <td>598952294131470701</td>\n",
       "      <td>202101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2782000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tipo_id tipo_id_homologado  no_identificacion          id_cliente  \\\n",
       "735871       1            1000003                206  421952294130855802   \n",
       "871475       1            1000003                357  702552294130987801   \n",
       "631622       1            1000003               1112  543052294130923801   \n",
       "656626       1            1000003               1272  962752294130988601   \n",
       "713245       1            1000003               1898  598952294131470701   \n",
       "\n",
       "        fecha_buro  numero_obligaciones_activasdif  porcentaje_utilizacion  \\\n",
       "735871      202101                        0.000000                  0.0000   \n",
       "871475      202101                       -0.166667                  2.1094   \n",
       "631622      202101                       -0.333333                  0.0000   \n",
       "656626      202101                        0.142857                  0.7924   \n",
       "713245      202101                        0.000000                  0.0000   \n",
       "\n",
       "        quanto_mod  valor_cuotas_codeudores_smlv  valor_utilizado_smlv  \n",
       "735871   3669000.0                           0.0              0.000000  \n",
       "871475   3432000.0                           0.0              1.973526  \n",
       "631622   1676000.0                           0.0              0.000000  \n",
       "656626  18188000.0                           0.0             66.737771  \n",
       "713245   2782000.0                           0.0              0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buro_mdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo_id</th>\n",
       "      <th>identificacion</th>\n",
       "      <th>fecha_pasivo</th>\n",
       "      <th>pasv_antig_total</th>\n",
       "      <th>pasv_num_meses_ult_apertura</th>\n",
       "      <th>pasv_dias_desde_ultima_trans</th>\n",
       "      <th>pasv_saldo_total_1mes_atras</th>\n",
       "      <th>pasv_saldo_ca_1mes_atras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>202102</td>\n",
       "      <td>37.533333</td>\n",
       "      <td>37.533333</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.822799</td>\n",
       "      <td>4.822799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>202103</td>\n",
       "      <td>38.466667</td>\n",
       "      <td>38.466667</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.638230</td>\n",
       "      <td>7.638230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>202104</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.221767</td>\n",
       "      <td>5.221767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1131</td>\n",
       "      <td>202102</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.383694</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1131</td>\n",
       "      <td>202103</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.767388</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tipo_id  identificacion  fecha_pasivo  pasv_antig_total  \\\n",
       "0        1             946        202102         37.533333   \n",
       "1        1             946        202103         38.466667   \n",
       "2        1             946        202104         39.500000   \n",
       "3        1            1131        202102          1.866667   \n",
       "4        1            1131        202103          2.800000   \n",
       "\n",
       "   pasv_num_meses_ult_apertura  pasv_dias_desde_ultima_trans  \\\n",
       "0                    37.533333                          54.0   \n",
       "1                    38.466667                          13.0   \n",
       "2                    39.500000                          44.0   \n",
       "3                     1.866667                           NaN   \n",
       "4                     2.800000                           NaN   \n",
       "\n",
       "   pasv_saldo_total_1mes_atras  pasv_saldo_ca_1mes_atras  \n",
       "0                     4.822799                  4.822799  \n",
       "1                     7.638230                  7.638230  \n",
       "2                     5.221767                  5.221767  \n",
       "3                     0.383694                  0.000000  \n",
       "4                     0.767388                  0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasvivos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cliente</th>\n",
       "      <th>fecha_camp</th>\n",
       "      <th>id_numero_cliente</th>\n",
       "      <th>id_tp_cd</th>\n",
       "      <th>periodo</th>\n",
       "      <th>data_camp</th>\n",
       "      <th>fecha_buro</th>\n",
       "      <th>fecha_activos</th>\n",
       "      <th>fecha_pasivo</th>\n",
       "      <th>fecha_lib</th>\n",
       "      <th>fecha_tx_pav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102652297589965301</td>\n",
       "      <td>202101</td>\n",
       "      <td>27079010</td>\n",
       "      <td>1000003</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>202105</td>\n",
       "      <td>202101</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102652296060855001</td>\n",
       "      <td>202101</td>\n",
       "      <td>14972564</td>\n",
       "      <td>1000003</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>202105</td>\n",
       "      <td>202101</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102652304429247101</td>\n",
       "      <td>202101</td>\n",
       "      <td>80354687</td>\n",
       "      <td>1000003</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>202105</td>\n",
       "      <td>202101</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102652295542372001</td>\n",
       "      <td>202101</td>\n",
       "      <td>11222544</td>\n",
       "      <td>1000003</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>202105</td>\n",
       "      <td>202101</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102652297598623801</td>\n",
       "      <td>202101</td>\n",
       "      <td>27076024</td>\n",
       "      <td>1000003</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>202105</td>\n",
       "      <td>202101</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202104</td>\n",
       "      <td>202101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_cliente  fecha_camp  id_numero_cliente id_tp_cd  periodo  \\\n",
       "0  102652297589965301      202101           27079010  1000003  04-2021   \n",
       "1  102652296060855001      202101           14972564  1000003  04-2021   \n",
       "2  102652304429247101      202101           80354687  1000003  04-2021   \n",
       "3  102652295542372001      202101           11222544  1000003  04-2021   \n",
       "4  102652297598623801      202101           27076024  1000003  04-2021   \n",
       "\n",
       "   data_camp  fecha_buro  fecha_activos  fecha_pasivo  fecha_lib  fecha_tx_pav  \n",
       "0     202105      202101         202104        202104     202104        202101  \n",
       "1     202105      202101         202104        202104     202104        202101  \n",
       "2     202105      202101         202104        202104     202104        202101  \n",
       "3     202105      202101         202104        202104     202104        202101  \n",
       "4     202105      202101         202104        202104     202104        202101  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poblacion_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('max_rows',None)\n",
    "\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def spark_read_parquet(s3_url: str, **args):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    # Leyendo base\n",
    "    dataset = pq.ParquetDataset(s3_url, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    dataframe = table.to_pandas()\n",
    "\n",
    "    del dataset, table\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "\"\"\"\n",
    "MODULE: processing_filtering\n",
    "This script extracts activos information from the activos file\n",
    "Steps:\n",
    "1. Get activos file\n",
    "2. Get the needed columns and rows from activos file including the needed historical period\n",
    "3. Create additional columns\n",
    "\"\"\"\n",
    "\n",
    "def processing_filtering(input_paths_activos):\n",
    "    \"\"\"Pending docstring\"\"\"\n",
    "    # 1. Get filter file\n",
    "    ids_final = get_filtering_file(input_paths_activos)\n",
    "    print('get_filtering_file')\n",
    "    # 2. Get demog file\n",
    "    data_demog = get_demog_file(input_paths_activos)\n",
    "    print('get_demog_file')\n",
    "    # 3. Join between \n",
    "    ids_final2 = pd.merge(ids_final,data_demog[['id_cliente','id_numero_cliente','id_tp_cd']], on='id_cliente', how='left')\n",
    "    print('ids_final_df')\n",
    "    print('filtering_df created successfully')\n",
    "\n",
    "    return ids_final2\n",
    "\n",
    "\n",
    "def get_filtering_file(input_path):\n",
    "    \"\"\" Gets the input_path to the activos file, drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param input_path to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    mes = '01'  ### REVISAR!!!!\n",
    "    anio = '2021'   ### REVISAR!!!!\n",
    "    \n",
    "    filtering_file_path = input_path['libranzas']\n",
    "    filtering = spark_read_parquet(filtering_file_path)\n",
    "\n",
    "    filtering.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "    \n",
    "    # Filter and casting variables\n",
    "    filtering = filtering[filtering['id_cliente'].notnull()]\n",
    "    filtering['id_cliente'] = filtering['id_cliente'].astype('int')\n",
    "    \n",
    "    ids = filtering.loc[~filtering['ds_estado_actual'].isin(['Cancelada','Castigado']),'id_cliente'].unique()\n",
    "    ids_final = pd.DataFrame({'id_cliente':ids})\n",
    "    ids_final['fecha_camp'] = int(anio+mes)  ### REVISAR!!!!\n",
    "\n",
    "    return ids_final\n",
    "\n",
    "\n",
    "def get_demog_file(input_path):\n",
    "    \"\"\" Gets the input_path to the activos file, drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param input_path to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # read the new table\n",
    "    data_demog = spark_read_parquet(input_path['clientes'])\n",
    "    data_demog = data_demog[data_demog['ref_num'].str.isdigit()]\n",
    "\n",
    "    # Join cedula, typeid and casting variables\n",
    "    data_demog = data_demog.rename(columns={'cont_id':'id_cliente',\n",
    "                                       'ref_num':'id_numero_cliente'})\n",
    "    data_demog['id_tp_cd'] = data_demog['id_tp_cd'].astype('str')\n",
    "    \n",
    "    # Drop duplicates and casting variables\n",
    "    data_demog = data_demog.drop_duplicates(subset=['id_tp_cd', 'id_numero_cliente'], keep='first')\n",
    "    data_demog['id_cliente'] = data_demog['id_cliente'].fillna('-99').astype(np.int64)\n",
    "\n",
    "    return data_demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3fs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_final2 = processing_filtering(input_paths_activos)\n",
    "\n",
    "print(ids_final2.shape)\n",
    "ids_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Activos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('max_rows',None)\n",
    "\n",
    "from datetime import datetime, timedelta \n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\"\"\"\n",
    "MODULE: processing_activos\n",
    "This script extracts activos information from the activos file\n",
    "Steps:\n",
    "1. Get activos file\n",
    "2. Get the needed columns and rows from activos file including the needed historical period\n",
    "3. Create additional columns\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def process_activos(input_paths_activos):\n",
    "    \"\"\"Pending docstring\"\"\"\n",
    "    # 1. Get activos file\n",
    "    activos = get_activos_file(input_paths_activos)\n",
    "    print('get_activos_file')\n",
    "    # 2. Deletes duplicates and casts variables\n",
    "    activos = get_activos_info(activos)\n",
    "    print('get_activo_info')\n",
    "    # 3. Creates aux variables requeried for the final explanatory variables\n",
    "    activos = create_activos_info(activos)\n",
    "    print('create_activos_info')\n",
    "    # 4. Aggregate and produce client-level output data frame\n",
    "    activos_cli_fec = get_activos_cliente(activos)\n",
    "    print('get_activos_cliente')\n",
    "    # 5. Produce client-level output data frame with explanatory variables\n",
    "    activos_final = create_activos_df(activos, activos_cli_fec)\n",
    "    print('create_activos_df')\n",
    "    print('activos_df created successfully')\n",
    "\n",
    "    return activos_final\n",
    "\n",
    "\n",
    "def get_activos_file(input_path):\n",
    "    \"\"\" Gets the input_path to the activos file, drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param input_path to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    activos_file_path = input_path['activo']\n",
    "    activos = spark_read_parquet(activos_file_path)\n",
    "\n",
    "    activos.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "\n",
    "    if activos['nroident'].isnull().sum() > 0:\n",
    "        activos = activos.loc[activos['nroident'].notnull()]\n",
    "\n",
    "    str_date = activos_file_path[0][-6:]\n",
    "    activos['fecha_activos'] = pd.to_datetime(str_date, format='%Y%m').strftime('%Y%m')\n",
    "\n",
    "    # cast nroident\n",
    "    activos['nroident'] = activos['nroident'].fillna('-999').astype(np.int64)\n",
    "    activos['tipo_ident_hom'] = activos['tipo_ident_hom'].fillna('-999').astype(np.int64)\n",
    "    activos = activos[\n",
    "        ['nroident','tipo_ident_hom', 'id_cliente', 'nrooblig', 'fecha_activos', 'dias_morak', 'lineacredi', 'producto', 'sal_capita',\n",
    "         'vlr_cuota', 'vlr_desemb']]\n",
    "\n",
    "    return activos\n",
    "\n",
    "\n",
    "def get_activos_info(activos):\n",
    "    \"\"\"Deletes duplicates and casts variables\n",
    "    :param activos to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # delete negative nroident\n",
    "    activos = activos[(activos['nroident'] > 0)]\n",
    "\n",
    "    # delete duplicates\n",
    "    activos = activos.sort_values(['nroident','tipo_ident_hom', 'fecha_activos'])\n",
    "    activos = activos.drop_duplicates(subset=['nroident','tipo_ident_hom', 'fecha_activos'], keep='first')\n",
    "\n",
    "    # casting variables\n",
    "    activos['fecha_activos'] = activos['fecha_activos'].astype(int)\n",
    "\n",
    "    return activos\n",
    "\n",
    "\n",
    "def create_activos_info(activos):\n",
    "    \"\"\"Creates aux variables requeried for the final explanatory variables\n",
    "    :param activos to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # Creates salario minimo mensual vigente\n",
    "    activos['ano_po'] = activos['fecha_activos'].astype(str).str[0:4].astype(int)\n",
    "    activos['smlv'] = np.where(activos['ano_po'] == 2018, 781242,\n",
    "                               np.where(activos['ano_po'] == 2019, 828116,\n",
    "                                        np.where(activos['ano_po'] == 2020, 877803,\n",
    "                                                 np.where(activos['ano_po'] == 2021, 908526,\n",
    "                                                 np.nan))))\n",
    "\n",
    "    # Creates active credits flag\n",
    "    #activos['cuenta_vigente'] = np.where(activos['estado'] == 'ACTIVO', 1, 0)\n",
    "    activos.loc[:, 'cuenta_vigente'] = 1\n",
    "\n",
    "    # Creates tipo_prod\n",
    "    activos['tipo_prod'] = np.where((activos['lineacredi'] == 61) & (activos['producto'] == 'TC'), 'TC_T',\n",
    "                                    np.where((activos['lineacredi'] == 62) & (activos['producto'] == 'TC'), 'TC_X',\n",
    "                                             activos['producto']))\n",
    "   \n",
    "    #Variable amortizacion a nivel obligación\n",
    "    activos['amortizacion'] = np.where((activos['vlr_desemb']> 0) & (activos['sal_capita'] >= 0), (activos['vlr_desemb']-activos['sal_capita'])/activos['vlr_desemb'],\n",
    "                                       np.where((activos['vlr_desemb'] > 0) & np.isnan(activos['sal_capita']),1,np.nan))\n",
    "\n",
    "    return activos\n",
    "\n",
    "\n",
    "def get_activos_cliente(activos):\n",
    "    \"\"\"Takes a product-level data frame and returns a client-level data frame,\n",
    "    :param activos complete activos DataFrame\n",
    "    :return: DataFrame, client-level\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates data base at 'nroident','fecha_activos' level.\n",
    "    activos_cli_fec = pd.DataFrame(activos.groupby(['nroident','tipo_ident_hom', 'fecha_activos'])[['smlv']].max())\n",
    "\n",
    "    return activos_cli_fec\n",
    "\n",
    "\n",
    "def create_activos_df(activos, activos_cli_fec):\n",
    "    \"\"\"Creates client-level final variables\n",
    "    :param activos complete activos DataFrame\n",
    "    :param activos_cli_fec containing activos relevant rows and columns and activos_cli_fec client level data base\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Numero de obligaciones libranza al dia\n",
    "    activos_cli_fec['amortizacion_min_lb'] = activos[\n",
    "        (activos['cuenta_vigente'] == 1) & (activos['producto'] == 'LB')].groupby(\n",
    "        ['nroident','tipo_ident_hom','fecha_activos'])[['amortizacion']].min()\n",
    "\n",
    "\n",
    "    # Saldo y cuota\n",
    "    activos_cli_fec['cuota_max_lb']  = activos[\n",
    "        (activos['producto'] == 'LB') & (activos['cuenta_vigente'] == 1)].groupby(\n",
    "        ['nroident','tipo_ident_hom','fecha_activos'])[['vlr_cuota']].max()\n",
    "\n",
    "    # Saldo y cuota / smlv\n",
    "    activos_cli_fec['cuota_max_lb_smlv'] = activos_cli_fec['cuota_max_lb'] / activos_cli_fec['smlv']\n",
    "\n",
    "    activos_cli_fec = activos_cli_fec.reset_index()\n",
    "    activos_out = activos_cli_fec[\n",
    "        ['nroident','tipo_ident_hom', 'fecha_activos', 'amortizacion_min_lb', 'cuota_max_lb_smlv']]\n",
    "    print('Shape inicial:',activos_out.shape[0])\n",
    "    activos_out = activos_out.loc[activos_out['amortizacion_min_lb'].notnull()]\n",
    "    print('Shape final:',activos_out.shape[0])\n",
    "   \n",
    "    return activos_out\n",
    "\n",
    "def spark_read_parquet(s3_url: str, **args):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    # Leyendo base\n",
    "    dataset = pq.ParquetDataset(s3_url, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    dataframe = table.to_pandas()\n",
    "\n",
    "    del dataset, table\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_activos = process_activos(input_paths_activos)\n",
    "# print(data_activos.shape)\n",
    "# data_activos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Campañas Activos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import boto3\n",
    "import boto3.session\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def process_camp_activos(input_paths_activos):\n",
    "    \"\"\"Pending docstring\"\"\"\n",
    "    \n",
    "    # Pre_crm. from different route\n",
    "    data_crm1 = get_crm_new_file(input_paths_activos)\n",
    "    print('get_crm_new_file')\n",
    "    # Pre_vend. from different route\n",
    "    data_vend1 = get_vend_new_file(input_paths_activos)\n",
    "    print('get_vend_new_file')\n",
    "    \n",
    "    # 1. Get crm file\n",
    "    data_crm = get_crm_file(input_paths_activos)\n",
    "    print('get_crm_file')\n",
    "    # 2. Deletes duplicates and casts variables\n",
    "    data_crm = get_crm_info(data_crm, data_crm1)\n",
    "    print('get_crm_info')\n",
    "    print(data_crm.head())\n",
    "    \n",
    "    # 3. Get vend file\n",
    "    data_vend = get_vend_file(input_paths_activos)\n",
    "    print('get_vend_file')\n",
    "    # 4. Deletes duplicates, casts variables and union data_crm - data_vend\n",
    "    data_camp = get_vend_info(data_vend, data_crm, data_vend1)\n",
    "    print('get_vend_info')\n",
    "    print(data_camp.head())\n",
    "    \n",
    "    # 5. Get activaciones file\n",
    "    data_act1 = get_activaciones_file(input_paths_activos)\n",
    "    print('get_activaciones_file')\n",
    "    # 6. Get activaciones_diarias file\n",
    "    data_act = get_acti_diarias_file(input_paths_activos)\n",
    "    print('get_acti_diarias_file')\n",
    "    # 7. Deletes duplicates and casts variables\n",
    "    data_act = get_act_diarias_info(data_act, data_act1)\n",
    "    print('get_act_diarias_info')\n",
    "    \n",
    "    # 8. Creates aux variables requeried for the final explanatory variables\n",
    "    base_final = create_func_activa(data_camp, data_act)\n",
    "    print('create_func_activa')\n",
    "    # 9. Deletes duplicates and casts variables\n",
    "    base_final = get_func_activa_info(base_final)\n",
    "    print('get_func_activa_info')\n",
    "    \n",
    "    # 10. Creates aux variables requeried for the final explanatory variables\n",
    "    tabla_final = create_func_var_camp_act(base_final)\n",
    "    print('create_func_var_camp_act')\n",
    "    # 11. Deletes duplicates and casts variables\n",
    "    tabla_final = get_func_var_camp_act(tabla_final)\n",
    "    print('get_func_var_camp_act')\n",
    "    print('camp_activos_df created successfully')  \n",
    "    \n",
    "    return tabla_final\n",
    "\n",
    "#########################################################\n",
    "################### Bases de campañas ###################\n",
    "#########################################################\n",
    "\n",
    "#####Prep. crm_new\n",
    "def get_crm_new_file(input_paths):\n",
    "    \n",
    "    path_file = get_s3_parquet_flag(input_paths['canpañas'], 0)[-1]\n",
    "    data_temp = pd.read_csv(path_file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "    data_temp = data_temp.sort_values(['base_name','date','clientid'])\n",
    "    data_temp = data_temp.groupby(['clientid','base_name']).first()\n",
    "    data_temp.reset_index(inplace=True)\n",
    "    \n",
    "    ## Filtrar express-la14\n",
    "    data_temp = data_temp.loc[~data_temp['base_name'].str.contains('EXPRES_LA14')]\n",
    "    data_temp = data_temp.rename(columns={'clientid':'NUM_DOC'})\n",
    "    \n",
    "    ## Oficinas\n",
    "    data_crm = data_temp.loc[~data_temp['base_name'].str.contains('OFICINA_BOGOTA')]\n",
    "    ## tipo campaña\n",
    "    data_crm['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_crm['tipo_camp'] = 'of-preaprobados'\n",
    "    # Casting variables\n",
    "    data_crm['FECHA'] = pd.to_datetime(data_crm['date'],format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(np.where(data_crm['FECHA'].dt.day>=28, \n",
    "                                                      data_crm['FECHA']+timedelta(days=10),\n",
    "                                                      data_crm['FECHA']),format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña2'] = pd.to_datetime(data_crm['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(data_crm['mes_campaña'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm1 = data_crm.copy()\n",
    "    \n",
    "    return data_crm1\n",
    "\n",
    "\n",
    "#####Prep. vend_new\n",
    "def get_vend_new_file(input_paths):\n",
    "    \n",
    "    path_file = get_s3_parquet_flag(input_paths['canpañas'], 0)[-1]\n",
    "    data_temp = pd.read_csv(path_file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "    data_temp = data_temp.sort_values(['base_name','date','clientid'])\n",
    "    data_temp = data_temp.groupby(['clientid','base_name']).first()\n",
    "    data_temp.reset_index(inplace=True)\n",
    "    \n",
    "    ## Filtrar express-la14\n",
    "    data_temp = data_temp.loc[~data_temp['base_name'].str.contains('EXPRES_LA14')]\n",
    "    data_temp = data_temp.rename(columns={'clientid':'NUM_DOC'})\n",
    "    \n",
    "    data_vend = data_temp.loc[~data_temp['base_name'].str.contains('CALL_ATENTO')]\n",
    "    # Casting variables\n",
    "    data_vend['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_vend['tipo_camp'] = 'at-preaprobados'\n",
    "    data_vend['FECHA'] = pd.to_datetime(data_vend['date'],format='%Y-%m-%d')\n",
    "    data_vend['mes_campaña'] = pd.to_datetime(data_vend['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_vend1 = data_vend.copy()\n",
    "    \n",
    "    return data_vend1\n",
    "\n",
    "\n",
    "#####1. OFICINAS\n",
    "\n",
    "def get_crm_file(input_paths):\n",
    "    files = input_paths['crm']\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        \n",
    "        data_temp = pd.read_csv(file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str}) \n",
    "        \n",
    "        if ix == 0:\n",
    "            data_crm = data_temp\n",
    "        else:\n",
    "            data_crm = pd.concat([data_crm,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "    \n",
    "    return data_crm\n",
    "\n",
    "## 2.    \n",
    "## tipo campaña\n",
    "def get_crm_info(data_crm, data_crm1):\n",
    "    \n",
    "    data_crm['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_crm['tipo_camp'] = 'of-preaprobados'\n",
    "    \n",
    "    # Casting variables\n",
    "    data_crm['FECHA'] = pd.to_datetime(data_crm['FECHA'],format='%d/%m/%Y')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(np.where(data_crm['FECHA'].dt.day>=28,\n",
    "                                                      data_crm['FECHA']+timedelta(days=10), \n",
    "                                                      data_crm['FECHA']),format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña2'] = pd.to_datetime(data_crm['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(data_crm['mes_campaña'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    data_crm = pd.concat([data_crm1, data_crm], ignore_index=True)\n",
    "    \n",
    "    return data_crm\n",
    "\n",
    "\n",
    "##### 3. ATENTO\n",
    "def get_vend_file(input_paths):\n",
    "\n",
    "    files = input_paths['vendors']\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        \n",
    "        data_temp = pd.read_csv(file, sep='|', encoding='ISO8859-1',quoting=1, dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "        if ix == 1:\n",
    "            data_temp['FECHA2'] = pd.to_datetime(data_temp['FECHA'],format='%d/%m/%Y')\n",
    "            data_temp = data_temp[data_temp['FECHA2']<'2020-12-01']\n",
    "            data_temp = data_temp[['TIP_DOC','NUM_DOC','ID_CLIENTE',' CUPO_APROBADO ','FECHA','TIPO CAMPAÑA']]\n",
    "        if ix == 0:\n",
    "            data_vend = data_temp\n",
    "        else:\n",
    "            data_vend = pd.concat([data_vend,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "    \n",
    "    return data_vend\n",
    "\n",
    "\n",
    "#4. Casting variables\n",
    "def get_vend_info(data_vend, data_crm, data_vend1):\n",
    "    \n",
    "    data_vend['tipo_camp'] = np.where(data_vend['TIPO CAMPAÑA'] == 'Preaprobados','at-preaprobados','at-perfilados')\n",
    "    data_vend['FECHA'] = pd.to_datetime(data_vend['FECHA'],format='%d/%m/%Y')\n",
    "    data_vend['mes_campaña'] = pd.to_datetime(data_vend['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    data_vend = pd.concat([data_vend1,data_vend], ignore_index=True)\n",
    "    \n",
    "    cols = ['NUM_DOC', 'FECHA', 'mes_campaña', 'TIPO CAMPAÑA', 'tipo_camp']\n",
    "    \n",
    "    data_vend.columns = data_vend.columns.str.strip()\n",
    "    data_camp = pd.concat([data_crm[cols], data_vend[cols]], ignore_index=True)[cols]\n",
    "        \n",
    "    data_camp['NUM_DOC'] = data_camp['NUM_DOC'].astype(np.int64)\n",
    "    data_camp['FECHA'] = pd.to_datetime(data_camp['FECHA'])\n",
    "    data_camp['mes_campaña'] = pd.to_datetime(data_camp['mes_campaña'])\n",
    "    data_camp = data_camp.rename(columns={'NUM_DOC':'cedula'})\n",
    "    \n",
    "    return data_camp\n",
    "\n",
    "    \n",
    "##5. ACTIVACIONES\n",
    "def get_activaciones_file(input_paths):\n",
    "\n",
    "    data_act2 = pd.read_csv(input_paths['activaciones'][0], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act2 = data_act2.drop_duplicates()\n",
    "    print(data_act2.shape)\n",
    "    \n",
    "    data_act1 = pd.read_csv(input_paths['activaciones'][1], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act1 = data_act1.drop_duplicates()\n",
    "    print(data_act1.shape)\n",
    "    \n",
    "    # Uniendo bases\n",
    "    data_act2['fecha_activacion'] = data_act2['fecha_activacion'].str.replace('/20','/2020')\n",
    "    data_act1 = pd.concat([data_act2,data_act1], ignore_index=True)\n",
    "    \n",
    "    # Casting variables\n",
    "    data_act1['fecha_activacion'] = pd.to_datetime(data_act1['fecha_activacion'],format='%d/%m/%Y')\n",
    "    data_act1['mes_activacion'] = pd.to_datetime(data_act1['fecha_activacion'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_act1.columns = data_act1.columns.str.lower()\n",
    "    data_act1 = data_act1.rename(columns={'identificacion':'cedula'})\n",
    "    del data_act2\n",
    "    \n",
    "    return data_act1\n",
    "\n",
    "\n",
    "##6. bases diarias\n",
    "def get_acti_diarias_file(input_paths):\n",
    "\n",
    "    files = get_s3_parquets(input_paths['acti_diarias'][0])\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        data_temp = spark_read_parquet(file)\n",
    "        if ix == 0:\n",
    "            data_act = data_temp\n",
    "        else:\n",
    "            data_act = pd.concat([data_act,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "        \n",
    "    return data_act\n",
    "\n",
    "        \n",
    "#7. Casting variables     \n",
    "def get_act_diarias_info(data_act, data_act1):\n",
    "\n",
    "    data_act['fecha_activacion_plastico'] = pd.to_datetime(data_act['fecha_activacion_plastico'])\n",
    "    data_act['mes_activacion'] = pd.to_datetime(data_act['fecha_activacion_plastico'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    ## Renombrando variables\n",
    "    data_act = data_act.rename(columns={'id_cli':'cedula',\n",
    "                                       'tipo_id_cliente':'tipo_identificacion',\n",
    "                                       'fecha_activacion_plastico':'fecha_activacion'})\n",
    "    \n",
    "    data_act = data_act[['numero_cuenta', 'numero_tarjeta', 'cedula', 'tipo_identificacion', 'fecha_activacion', 'id_cliente',\n",
    "           'mes_activacion']]\n",
    "    \n",
    "    ## Uniendo bases\n",
    "    data_act = pd.concat([data_act1, data_act], ignore_index=True)\n",
    "    data_act = data_act.drop_duplicates()\n",
    "\n",
    "    del data_act1\n",
    "\n",
    "    ## Cast variables\n",
    "    data_act['id_cliente'] = data_act['id_cliente'].fillna('-9999').astype(np.int64)\n",
    "    data_act['tipo_identificacion'] = data_act['tipo_identificacion'].fillna('-9999').astype(np.int64)\n",
    "    data_act['cedula'] = data_act['cedula'].astype(np.int64)\n",
    "\n",
    "    return data_act\n",
    "\n",
    "\n",
    "###8. func_activa\n",
    "def create_func_activa(data_camp, data_act):\n",
    "\n",
    "    str_dates = ['2019-01','2019-02','2019-03','2019-04','2019-05','2019-06','2019-07','2019-08','2019-09','2019-10','2019-11',\n",
    "                 '2019-12','2020-01','2020-02','2020-03','2020-04','2020-05','2020-06','2020-07','2020-08','2020-09','2020-10', \n",
    "                 '2020-11','2020-12','2021-01','2021-02','2021-03','2021-04','2021-05','2021-06']\n",
    "    n_mes = 3 # numero de meses\n",
    "    \n",
    "    for ix, date in enumerate(str_dates):\n",
    "        print('Fecha:',date)\n",
    "        date = pd.to_datetime(date,format='%Y-%m')\n",
    "        nom_col = 'camp_'+date.strftime('%Y%m')\n",
    "        \n",
    "        base = data_camp[data_camp['mes_campaña']==date]\n",
    "    \n",
    "        if ix == 0:\n",
    "            base_final = func_activa(base, data_act, date, n_mes, nom_col)\n",
    "            del base\n",
    "        else:\n",
    "            base_temp = func_activa(base, data_act, date, n_mes, nom_col)\n",
    "            base_final = pd.concat([base_final, base_temp], ignore_index=True)\n",
    "            \n",
    "            del base, base_temp\n",
    "\n",
    "    return base_final\n",
    "\n",
    "\n",
    "#9. Casting variables\n",
    "def get_func_activa_info(base_final):\n",
    "    \n",
    "    base_final = base_final[base_final['cedula'].notnull()]\n",
    "    \n",
    "    # base_final.rename(columns={' CUPO_APROBADO ':'CUPO_APROBADO'}, inplace=True)\n",
    "    # base_final['CUPO_APROBADO'] = base_final['CUPO_APROBADO'].str.strip()\n",
    "    # base_final['CUPO_APROBADO'] = [str(x).replace('.','') for x in base_final['CUPO_APROBADO']]\n",
    "    # base_final['CUPO_APROBADO'] = base_final['CUPO_APROBADO'].astype('float')\n",
    "    \n",
    "    cols = ['camp_201901','camp_201902','camp_201903','camp_201904','camp_201905','camp_201906','camp_201907',\n",
    "            'camp_201908','camp_201909','camp_201910','camp_201911','camp_201912','camp_202001','camp_202002',\n",
    "            'camp_202003','camp_202004','camp_202005','camp_202006','camp_202008','camp_202009','camp_202010',\n",
    "            'camp_202011','camp_202012','camp_202101','camp_202102','camp_202103','camp_202104','camp_202105',\n",
    "            'camp_202106','venta']\n",
    "    \n",
    "    base_final.loc[:,cols] = base_final[cols].fillna(0)\n",
    "    \n",
    "    # Casting variables\n",
    "    base_final['fecha_activacion'] = pd.to_datetime(base_final['fecha_activacion'], format='%Y-%m%d')\n",
    "    base_final['mes_activacion'] = pd.to_datetime(base_final['mes_activacion'], format='%Y-%m%d')\n",
    "\n",
    "    return base_final\n",
    "\n",
    "##10 Crear variable meses\n",
    "def create_func_var_camp_act(base_final):\n",
    "\n",
    "    str_dates = ['2019-02','2019-03','2019-04','2019-05','2019-06','2019-07','2019-08','2019-09','2019-10','2019-11',\n",
    "                 '2019-12','2020-01','2020-02','2020-03','2020-04','2020-05','2020-06','2020-07','2020-08','2020-09',\n",
    "                 '2020-10','2020-11','2020-12','2021-01','2021-02','2021-03','2021-04','2021-05','2021-06']\n",
    "    \n",
    "    for ix, dates in enumerate(str_dates):\n",
    "        print('Periodo:', dates)\n",
    "        date = pd.to_datetime(dates,format='%Y-%m')\n",
    "        \n",
    "        ids = list(base_final.loc[base_final['mes_campaña']==date,'cedula'].unique())\n",
    "        gp_temp = base_final[base_final['mes_campaña']<date].groupby('cedula')\n",
    "        # gp_temp_ap = base_final[(base_final['mes_campaña']<date)&(base_final['CUPO_APROBADO'].notnull())].groupby('cedula')\n",
    "        \n",
    "        if ix == 0:\n",
    "            tabla_final = func_var_camp_act(gp_temp, ids, date)\n",
    "            \n",
    "        #    if len(gp_temp_ap)>0:\n",
    "        #        tabla_final['prom_cupo_aprob'] = gp_temp_ap['CUPO_APROBADO'].mean()\n",
    "        \n",
    "            tabla_final.index.rename('cedula', inplace=True)\n",
    "            tabla_final.reset_index(inplace = True)\n",
    "        else:\n",
    "            base_temp = func_var_camp_act(gp_temp, ids, date)\n",
    "            \n",
    "        #    if len(gp_temp_ap)>0:\n",
    "        #        base_temp['prom_cupo_aprob'] = gp_temp_ap['CUPO_APROBADO'].mean()\n",
    "        \n",
    "            base_temp.index.rename('cedula', inplace=True)\n",
    "            base_temp.reset_index(inplace = True)\n",
    "            \n",
    "            tabla_final = pd.concat([tabla_final,base_temp], ignore_index=True)\n",
    "            \n",
    "            del base_temp\n",
    "        del gp_temp, date\n",
    "    \n",
    "    return tabla_final\n",
    "\n",
    "    \n",
    "##11. Cast index\n",
    "def get_func_var_camp_act(tabla_final):\n",
    "\n",
    "    tabla_final['cedula'] = tabla_final['cedula'].fillna('-999')\n",
    "    tabla_final['cedula'] = tabla_final['cedula'].astype('int')\n",
    "    \n",
    "    tabla_final= tabla_final[['cedula', 'mes_campaña','acep_oferta_prev', 'mes_ult_act',\n",
    "                              'mes_ult_camp', 'num_act_utl_meses', 'num_camp_ult_meses',\n",
    "                              'num_meses_ult_actv', 'num_meses_ult_camp', 'num_no_aceptado']]\n",
    "    \n",
    "    tabla_final = tabla_final[(tabla_final['cedula'].notnull()) & (tabla_final['mes_campaña'].notnull())]\n",
    "    tabla_final = tabla_final.drop_duplicates()\n",
    "    \n",
    "    tabla_final['data_camp'] = pd.to_datetime(tabla_final['mes_campaña'], format='%Y-%m-%d').dt.strftime('%Y%m').astype('int')\n",
    "    tabla_final = tabla_final.sort_values(['cedula', 'mes_campaña'])\n",
    "    \n",
    "    return tabla_final\n",
    "\n",
    "\n",
    "######## Funciones utilizadas\n",
    "    \n",
    "def spark_read_parquet(s3_url: str, **args):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    # Leyendo base\n",
    "    dataset = pq.ParquetDataset(s3_url, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    dataframe = table.to_pandas()\n",
    "\n",
    "    del dataset, table\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def func_activa(base, data_act, date, n_mes, nom_col):\n",
    "    if len(base)>0:\n",
    "        date_fin = date + relativedelta(months=n_mes)\n",
    "\n",
    "        data_act_temp = data_act.loc[(data_act['mes_activacion']>=date) & (data_act['mes_activacion']<=date_fin),['cedula','Numero_cuenta','fecha_activacion','mes_activacion']].drop_duplicates()\n",
    "        ids_mes = data_act_temp['cedula'].unique()\n",
    "        \n",
    "        base.loc[base['cedula'].isin(ids_mes),'venta'] = 1    \n",
    "        base.loc[base['venta'].isnull(),'venta'] = 0\n",
    "        \n",
    "        base.loc[:,nom_col] = 1   \n",
    "        #base = pd.merge(base,data_act_temp,on=['cedula','ID_CLIENTE'],how='left')\n",
    "        base = pd.merge(base,data_act_temp,on=['cedula'],how='left')\n",
    "    \n",
    "    return base\n",
    "\n",
    "def func_var_camp_act(gp_temp, ids, date):\n",
    "    \n",
    "    #producing the first dataframe\n",
    "    base_out = pd.DataFrame(index=ids) # this is different from the actual prod code\n",
    "    base_out['mes_campaña'] = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    ## Calculando días primer uso\n",
    "    base_out['mes_ult_camp'] =  gp_temp['mes_campaña'].max()\n",
    "    base_out['mes_ult_act'] =  gp_temp['fecha_activacion'].max()\n",
    "    base_out.loc[base_out['mes_ult_camp'].notnull(),'num_meses_ult_camp'] = (date - base_out.loc[base_out['mes_ult_camp'].notnull(),'mes_ult_camp'])/pd.offsets.Day(30)\n",
    "    base_out.loc[base_out['mes_ult_act'].notnull(),'num_meses_ult_actv'] = (date - base_out.loc[base_out['mes_ult_act'].notnull(),'mes_ult_act'])/pd.offsets.Day(30)\n",
    "    \n",
    "    base_out['num_camp_ult_meses'] = gp_temp['mes_campaña'].count()\n",
    "    base_out['num_act_utl_meses'] = gp_temp['venta'].sum()\n",
    "    base_out['num_no_aceptado'] = gp_temp['venta'].value_counts().unstack()[0].fillna(0).astype(int)\n",
    "    base_out['acep_oferta_prev'] = gp_temp['venta'].max()\n",
    "    \n",
    "    return base_out\n",
    "\n",
    "def _key_existing_size__list(client, bucket, key):\n",
    "    \"\"\"return the key's size if it exist, else None\"\"\"\n",
    "    response = client.list_objects_v2(Bucket=bucket, Prefix=key)\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        if obj[\"Key\"] == key:\n",
    "            return obj[\"Size\"]\n",
    "        \n",
    "def _key_files__list(client, bucket, key):\n",
    "    \"\"\"return the key's size if it exist, else None\"\"\"\n",
    "    response = client.list_objects_v2(Bucket=bucket, Prefix=key)\n",
    "    return response.get(\"Contents\", [])\n",
    "\n",
    "def parquet_exists(s3_url):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\"s3\")\n",
    "    bucket = s3_url.split(\"/\")[2]\n",
    "    key = s3_url.split(\"/\")[3:]\n",
    "    key = \"/\".join(key)+ \"/_SUCCESS\"\n",
    "    size = _key_existing_size__list(s3, bucket, key)\n",
    "    if size is not None:\n",
    "        print(\"File exists:\", s3_url)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_s3_parquets(s3_url):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\"s3\")\n",
    "    bucket = s3_url.split(\"/\")[2]\n",
    "    key = s3_url.split(\"/\")[3:]\n",
    "    key = \"/\".join(key)\n",
    "    objects = _key_files__list(s3, bucket, key)\n",
    "    files = [\"s3://\" + bucket + \"/\" + file[\"Key\"][:-9] for file in objects if file[\"Key\"].split(\"/\")[-1]==\"_SUCCESS\"]\n",
    "    return files\n",
    "\n",
    "def get_s3_parquet_flag(s3_url, flag):\n",
    "    \"\"\"\n",
    "    s3_url: path donde se desea buscar\n",
    "    flag: permite identificar por cual camino entrar: 1: buscar archivo especifico, 0: buscar carpeta, ejemplo las std\n",
    "    \"\"\"\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\"s3\")\n",
    "    bucket = s3_url.split(\"/\")[2]\n",
    "    print('bucket_last_file: ', bucket)\n",
    "    key = s3_url.split(\"/\")[3:]\n",
    "    key = \"/\".join(key)\n",
    "    print('key_last_file: ', key)\n",
    "    files = _key_files__list(s3, bucket, key)\n",
    "    files = [\"s3://\" + bucket + \"/\" + file[\"Key\"] for file in files]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabla_final = process_camp_activos(input_paths_activos)\n",
    "# print(tabla_final.shape)\n",
    "# tabla_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proccess Buro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODULE: processing_buro\n",
    "This script extracts Buró information from the Buró file\n",
    "Steps:\n",
    "1. Get buro files\n",
    "2. Get the needed columns and rows from Buro file including the needed historical period\n",
    "3. Create additional columns\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_buro(input_paths_activos):\n",
    "    \"\"\"Using the input_paths for the last two buro files, this function puts everything together\"\"\"\n",
    "\n",
    "    # 1. Gets and appends the last two buro files\n",
    "    buro = get_buro_file(input_paths_activos)\n",
    "    print('get_buro_file')\n",
    "    # 2. Get the needed columns and rows from buro file including the needed historical period\n",
    "    buro = get_buro_info(buro)\n",
    "    print('get_buro_info')\n",
    "    # 3. Perform simple imputation to output data frame\n",
    "    buro_final = create_buro_df(buro)\n",
    "    print('create_buro_df')\n",
    "    print('buro_df created successfully')\n",
    "\n",
    "    return buro_final\n",
    "\n",
    "\n",
    "def get_buro_file(input_path):\n",
    "    \"\"\" Gets the input_paths of the last two the buro files, appends them and drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param\n",
    "    input_path1: location of the oldest buro file\n",
    "    input path2: location of the most recent buro file\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    input_path2 = get_s3_parquets(input_path['buro'])[-2]\n",
    "    \n",
    "    buro_1 = spark_read_parquet(input_path2)\n",
    "    buro_1.columns = buro_1.columns.str.lower()\n",
    "    buro_1 = buro_1.rename(columns={'cont_id': 'id_cliente'})\n",
    "\n",
    "    buro_1['marca_buro'] = 1\n",
    "    if buro_1['no_identificacion'].isnull().sum() > 0:\n",
    "        buro_1 = buro_1.loc[buro_1['no_identificacion'].notnull()]\n",
    "\n",
    "    buro_1['no_identificacion'] = buro_1['no_identificacion'].astype('str')\n",
    "    buro_1['no_identificacion'] = buro_1['no_identificacion'].astype(np.int64)\n",
    "\n",
    "    input_path1 = get_s3_parquets(input_path['buro'])[-1]\n",
    "    buro_2 = spark_read_parquet(input_path1)\n",
    "    buro_2.columns = buro_2.columns.str.lower()\n",
    "    buro_2 = buro_2.rename(columns={'cont_id': 'id_cliente'})\n",
    "\n",
    "    buro_2['marca_buro'] = 2\n",
    "    if buro_2['no_identificacion'].isnull().sum() > 0:\n",
    "        buro_2 = buro_2.loc[buro_2['no_identificacion'].notnull()]\n",
    "\n",
    "    buro_2['no_identificacion'] = buro_2['no_identificacion'].astype('str')\n",
    "    buro_2['no_identificacion'] = buro_2['no_identificacion'].astype(np.int64)\n",
    "\n",
    "    buro = pd.concat([buro_1,buro_2], ignore_index=True)\n",
    "    \n",
    "    if buro['no_identificacion'].isnull().sum() > 0:\n",
    "        buro = buro.loc[buro['no_identificacion'].notnull()]\n",
    "\n",
    "    buro['no_identificacion'] = buro['no_identificacion'].astype(np.int64)\n",
    "    buro['id_cliente'] = buro['id_cliente'].fillna('-999').astype(np.int64)\n",
    "    buro = buro[['tipo_id','tipo_id_homologado','no_identificacion', 'fecha_envio',\n",
    "                 'id_cliente', 'marca_buro', \n",
    "                 'numero_obligaciones_activas',\n",
    "                 'porcentaje_utilizacion',\n",
    "                 'quanto_mod',\n",
    "                 'valor_cupos',\n",
    "                 'valor_utilizado',\n",
    "                 'valor_cuotas_codeudores']]\n",
    "    \n",
    "    buro = buro.fillna(0)\n",
    "\n",
    "    return buro\n",
    "\n",
    "\n",
    "def get_buro_info(buro):\n",
    "    \"\"\"Deletes duplicates and casts dates\n",
    "    :param buro to buro file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # casting dates\n",
    "    buro = buro[buro['fecha_envio'].notnull()]\n",
    "    buro['fecha_envio'] = buro['fecha_envio'].astype('str')\n",
    "    buro['fecha_buro'] = (\n",
    "                buro['fecha_envio'].str.replace('-', '').str[0:4] + buro['fecha_envio'].str.replace('-', '').str[\n",
    "                                                                    4:6]).astype('int')\n",
    "\n",
    "    # deleting duplicates\n",
    "    buro = buro.sort_values(['no_identificacion', 'fecha_buro'])\n",
    "    buro = buro.drop_duplicates(subset=['no_identificacion', 'fecha_buro'], keep='first')\n",
    "\n",
    "    return buro\n",
    "\n",
    "\n",
    "def create_buro_df(buro):\n",
    "    \"\"\"Creates client-level variables final variables\n",
    "    :param buro containing buro relevant rows and columns\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # creating lag variables\n",
    "    buro['llave_lag'] = np.where(buro['no_identificacion'] == buro['no_identificacion'].shift(1), 1, 0)\n",
    "\n",
    "    # variable cupo mercado\n",
    "    buro['ano_po'] = buro['fecha_buro'].astype(str).str[0:4].astype(int)\n",
    "    buro['smlv'] = np.where(buro['ano_po'] == 2018, 781242,\n",
    "                            np.where(buro['ano_po'] == 2019, 828116,\n",
    "                                     np.where(buro['ano_po'] == 2020, 877803,\n",
    "                                              np.where(buro['ano_po'] == 2021, 908526,\n",
    "                                              np.nan))))\n",
    "    \n",
    "    if (buro['porcentaje_utilizacion'] > 0).sum()/buro.shape[0] > 0.9:\n",
    "    \n",
    "        buro['porcentaje_utilizacion'] = [float(str(x).replace('\"','').replace(',','.')) for x in buro['porcentaje_utilizacion']]\n",
    "        buro['porcentaje_utilizacion'] = buro['porcentaje_utilizacion'].astype('float')\n",
    "    else:\n",
    "        buro['porcentaje_utilizacion'] = np.where(buro['valor_cupos']>0, round(buro['valor_utilizado']/buro['valor_cupos'],4),0) \n",
    "\n",
    "\n",
    "    buro['valor_cuotas_codeudores_smlv']= buro['valor_cuotas_codeudores']/buro['smlv']\n",
    "    buro['valor_utilizado_smlv']= buro['valor_utilizado']/buro['smlv']\n",
    "    \n",
    "    var = 'numero_obligaciones_activas'\n",
    " \n",
    "    # creating lag variables lag\n",
    "    buro['llave_lag'] = np.where(buro['no_identificacion'] == buro['no_identificacion'].shift(1), 1, 0)\n",
    "\n",
    "    buro[var+'lag1']=np.where(buro['llave_lag']==1,buro[var].shift(1),np.nan)\n",
    "    buro[var+'dif']=np.where( ((np.isnan(buro[var])) | (buro[var]==0)) & ((np.isnan(buro[var+'lag1'])) | (buro[var+'lag1']==0)) ,0,\n",
    "                              np.where( ((np.isnan(buro[var])) | (buro[var]==0)) & ((buro[var+'lag1']>0)) ,-1,\n",
    "                              np.where( ((np.isnan(buro[var+'lag1'])) | (buro[var+'lag1']==0)) & ((buro[var]>0)) ,1,\n",
    "                              np.where( (buro[var+'lag1']>0) & (buro[var]>0) , (buro[var]-buro[var+'lag1'])/buro[var+'lag1'],99))))\n",
    "    \n",
    "    del buro[var+'lag1'], var\n",
    "    \n",
    "    #Last filter\n",
    "    buro = buro[buro['marca_buro'] == 2]\n",
    "    \n",
    "    buro_out = buro[['tipo_id','tipo_id_homologado','no_identificacion','id_cliente', 'fecha_buro', \n",
    "                     'numero_obligaciones_activasdif',\n",
    "                    'porcentaje_utilizacion',\n",
    "                    'quanto_mod',\n",
    "                    'valor_cuotas_codeudores_smlv',\n",
    "                    'valor_utilizado_smlv']]\n",
    "\n",
    "    return buro_out\n",
    "\n",
    "def spark_read_parquet(s3_url: str, **args):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    # Leyendo base\n",
    "    dataset = pq.ParquetDataset(s3_url, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    dataframe = table.to_pandas()\n",
    "\n",
    "    del dataset, table\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buro_mdt = process_buro(input_paths_activos)\n",
    "# print(buro_mdt.shape)\n",
    "# buro_mdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Libranzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import calendar\n",
    "import gc\n",
    "\n",
    "pd.set_option('max_columns',None)\n",
    "pd.set_option('max_rows',None)\n",
    "\n",
    "\"\"\"\n",
    "module: processing_libranzas\n",
    "This script extracts libranzas information from the libranzas file\n",
    "Steps:\n",
    "1. Get libranzas file\n",
    "2. Get the needed columns and rows from libranzas file including the needed historical period\n",
    "3. Create additional columns\n",
    "4. Aggregate and produce client-level output data frame\n",
    "5. Perform simple imputation to output data frame\n",
    "\"\"\"\n",
    "\n",
    "def process_libranzas(input_paths_activos: str) -> pd.DataFrame:\n",
    "    \"\"\"Using the input_path this function puts everything together\"\"\"\n",
    "    # 1. Get libranzas file\n",
    "    libranzas = get_libranzas_file(input_paths_activos)\n",
    "    print('get_libranzas_file')\n",
    "    # 2. Get the needed columns and rows from libranzas file including the needed historical period\n",
    "    libranzas = get_libranzas_info(libranzas)\n",
    "    print('get_libranzas_info')\n",
    "    # 3. Create additional columns\n",
    "    libranzas = add_columns_to_lib(libranzas)\n",
    "    print('add_columns_to_lib')\n",
    "    # 4. Aggregate and produce client-level output data frame\n",
    "    libranzas = create_lib_df(libranzas)\n",
    "    print('create_lib_df')\n",
    "    # 5. Perform simple imputation to output data frame\n",
    "    libranzas_final = simple_imputation(libranzas)\n",
    "    \n",
    "    libranzas_final['periodo'] = int(input_paths_activos['libranzas'][0][-6:])\n",
    "    libranzas_final.reset_index(inplace=True)\n",
    "    libranzas_final = libranzas_final.rename(columns={'id_cli':'id_numero_cliente','standard_id':'id_tp_cd'})\n",
    "    print('libranzas_df created successfully')\n",
    "    return libranzas_final\n",
    "\n",
    "\n",
    "def get_libranzas_file(input_path):\n",
    "    \"\"\" Gets the input_path to the libranzas file, drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param input_path to libranzas file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    file = input_path['libranzas']\n",
    "    dataset = pq.ParquetDataset(file, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    lib = table.to_pandas()\n",
    "    lib.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "    if lib['id_cli'].isnull().sum() > 0:\n",
    "        lib = lib.loc[lib['id_cli'].notnull()]\n",
    "\n",
    "    lib['id_cli'] = lib['id_cli'].astype(np.int64)\n",
    "    lib['standard_id'] = lib['standard_id'].astype(np.int64)\n",
    "    \n",
    "    lib_constant_cols = ['sk_producto_servicio', 'cd_modalidad_pag_int',\n",
    "                         'ds_modalidad_pag_int', 'cd_periodicidad_pag_int',\n",
    "                         'ds_periodicidad_pag_int', 'cd_base_liquidacion',\n",
    "                         'no_obligacion_novada']\n",
    "\n",
    "    lib.drop(columns=lib_constant_cols, inplace=True)\n",
    "    \n",
    "    del table, dataset\n",
    "\n",
    "    return lib\n",
    "\n",
    "\n",
    "def get_libranzas_info(lib):\n",
    "    \"\"\"Gets libranzas relevant rows and columns\n",
    "    :param lib to libranzas file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    HISTORICAL_YEARS = 13\n",
    "    # we are working only with \"libranzas organicas\"\n",
    "    lib = lib.loc[lib['ds_tipo_libranza'] == 'Organica']\n",
    "\n",
    "    # eliminating these records, they may have quality issues (less than 1%)\n",
    "    lib = lib.loc[lib['ds_tipo_credito'].notnull()]\n",
    "\n",
    "    # necessary date processing\n",
    "    lib_date_cols = ['fe_solicitud', 'fe_desembolso']\n",
    "    for col in lib_date_cols:\n",
    "        lib[col] = pd.to_datetime(lib[col], dayfirst=True, errors='coerce')\n",
    "\n",
    "    # getting records for the appropriate period\n",
    "    end_historical_period = pd.Timestamp(get_prev_months_last_date())\n",
    "    number_of_years = datetime.timedelta(days=int(365.25*HISTORICAL_YEARS))\n",
    "    beginning_historical_period = end_historical_period - number_of_years\n",
    "    period_filter = (lib['fe_solicitud'] >= beginning_historical_period) & (lib['fe_solicitud'] <= end_historical_period)\n",
    "    lib = lib.loc[period_filter]\n",
    "\n",
    "    # imputing fe_solicitud missing dates (less than 0.5%)\n",
    "    lib.loc[lib['fe_solicitud'].isnull(), 'fe_solicitud'] = lib.loc[lib['fe_solicitud'].isnull(), 'fe_desembolso']\n",
    "\n",
    "    # getting only the relevant columns\n",
    "    lib_relevant_cols = ['id_cli','standard_id', 'sk_rc_libranza', 'fe_solicitud',\n",
    "                         'vl_monto_solicitado', 'fe_desembolso',\n",
    "                         'vl_monto_aprobado', 'no_obligacion', 'vl_monto_desembolsado',\n",
    "                         'no_cuotas', 'vl_total_cuota', 'ds_estado_actual',\n",
    "                         'vl_tasa', 'ds_tipo_credito', 'ds_tipo_libranza', 'vl_monto_novado']\n",
    "\n",
    "    lib = lib[lib_relevant_cols]\n",
    "    lib = lib.loc[~lib['ds_estado_actual'].isin(['Cancelada','Castigado']),:]\n",
    "\n",
    "    return lib\n",
    "\n",
    "\n",
    "def add_columns_to_lib(lib):\n",
    "    \"\"\"Takes a libranzas DataFrame and adds some useful columns\n",
    "    :param lib DataFrame containing libranzas relevant rows and columns\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    lib['dias_procesamiento'] = (lib['fe_desembolso'] - lib['fe_solicitud'])/np.timedelta64(1, 'D')\n",
    "    # this is a necessary fix because there are many dates with fecha de solicitud in 1900\n",
    "    lib.loc[lib['dias_procesamiento'] > 360, 'dias_procesamiento'] = lib['dias_procesamiento'].median()\n",
    "    lib.loc[(lib['vl_monto_aprobado'] == 0)|(lib['vl_monto_aprobado'].isnull()),'vl_monto_aprobado'] = lib['vl_monto_desembolsado']\n",
    "\n",
    "    return lib\n",
    "\n",
    "\n",
    "def create_lib_df(lib):\n",
    "    \"\"\"Creates client-level variables by aggregating the columns\n",
    "    :param lib containing libranzas relevant rows and columns\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # producing the first data frame\n",
    "    gp = lib.groupby(['id_cli','standard_id'])\n",
    "    lib_out = lib[['id_cli','standard_id']].drop_duplicates()\n",
    "    lib_out.set_index(['id_cli','standard_id'], inplace=True)\n",
    "\n",
    "    # number of records for each cliente\n",
    "    lib_out['num_lib_solicitadas'] = gp.size()\n",
    "\n",
    "    # these have a very close relation\n",
    "    lib_out['prom_monto_novado'] = gp['vl_monto_novado'].mean()\n",
    "\n",
    "    # other averages\n",
    "    lib_out['prom_n_cuotas'] = gp['no_cuotas'].mean()\n",
    "\n",
    "    return lib_out\n",
    "\n",
    "\n",
    "def get_prev_months_last_date():\n",
    "    \"\"\"From current date returns the last date of the previous month\"\"\"\n",
    "    current_date = datetime.date.today()\n",
    "    first_day_of_month = current_date.replace(day=1)\n",
    "    out = first_day_of_month - datetime.timedelta(days=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_prev_prev_months_last_date():\n",
    "    \"\"\"From current date returns the last date of the month before the previous month\"\"\"\n",
    "    last_day_of_prev_month = get_prev_months_last_date()\n",
    "    first_day_of_prev_month = last_day_of_prev_month.replace(day=1)\n",
    "    out = first_day_of_prev_month - datetime.timedelta(days=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def simple_imputation(df):\n",
    "    \"\"\"Gets a data frame and imputes the median for numerical columns and the\n",
    "    mode (most common category) for categorical columns\"\"\"\n",
    "    numerical_cols = df.select_dtypes(np.number).columns\n",
    "    categorical_cols = df.select_dtypes(object).columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libranzas_df = process_libranzas(input_paths_activos)\n",
    "# print(libranzas_df.shape)\n",
    "# libranzas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Pasivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "pd.set_option('max_columns',None)\n",
    "#pd.set_option('max_rows',100)\n",
    "\n",
    "from datetime import datetime, timedelta \n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "\"\"\"\n",
    "MODULE: processing_activos\n",
    "This script extracts activos information from the activos file\n",
    "Steps:\n",
    "1. Get activos file\n",
    "2. Get the needed columns and rows from activos file including the needed historical period\n",
    "3. Create additional columns\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def process_pasivos(input_paths_activos):\n",
    "    \"\"\"Pending docstring\"\"\"\n",
    "    # 1. Get activos file\n",
    "    pasivos = get_pasivos_file(input_paths_activos)\n",
    "    print('get_pasivos_file')\n",
    "    \n",
    "    # 2. Deletes duplicates and casts variables\n",
    "    pasivos = get_pasivos_info(pasivos)\n",
    "    print('get_pasivos_info')\n",
    "     \n",
    "    # 3. Creates aux variables requeried for the final explanatory variables\n",
    "    pasivos_last_3 = get_pasivos_last_3(pasivos)\n",
    "    print('get_pasivos_last_3')\n",
    "    \n",
    "    # 4. Aggregate and produce client-level output data frame\n",
    "    pasivos_last_3 = create_pasivo_last_3_info(pasivos_last_3)\n",
    "    print('create_pasivo_last_3_info')\n",
    "    \n",
    "    # 5. Produce client-level output data frame with explanatory variables\n",
    "    pasivos_last_3_cli_fec = create_pasivos_df(pasivos_last_3)\n",
    "    print('create_pasivos_last_3_cli_df')\n",
    "    \n",
    "    6. \n",
    "    pasv_out = create_pasv_out(pasivos_last_3_cli_fec)\n",
    "    print('create_pasv_out')\n",
    "    \n",
    "    # 7. \n",
    "    pasvivos_df = pasivo_last_3_tot(pasivos_last_3_cli_fec, pasv_out)\n",
    "    print('pasivo_last_3_tot')   \n",
    "    print('pasivos_df created successfully')\n",
    "\n",
    "    return pasvivos_df\n",
    "\n",
    "\n",
    "\n",
    "def get_pasivos_file(input_path):\n",
    "    \"\"\" Gets the input_path to the activos file, drops some not useful columns\n",
    "     and outputs a DataFrame\n",
    "    :param input_path to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    product_type_dict = {'1':'corriente', '2':'ahorro', '3':'cdt'}\n",
    "    str_dates = ['202102','202103','202104'] ### vector_fechas\n",
    "    \n",
    "    for ix, date in enumerate(str_dates):\n",
    "\n",
    "        tabla_temp = creando_pasivos_antg(input_path, date)\n",
    "        tabla_temp.columns = tabla_temp.columns.str.lower()\n",
    "\n",
    "        ## Cambiar tipo producto:\n",
    "        tabla_temp['periodo'] = int(date)\n",
    "        tabla_temp['tipo_producto'] = tabla_temp['cod_producto'].apply(lambda x: str(x)[0]).map(product_type_dict)\n",
    "\n",
    "        print(tabla_temp.shape)\n",
    "\n",
    "        print('Periodo:',date)\n",
    "        if ix == 0:\n",
    "            tabla_final = tabla_temp\n",
    "        else:\n",
    "            tabla_final = pd.concat([tabla_final,tabla_temp], ignore_index=True)\n",
    "\n",
    "        del tabla_temp\n",
    "\n",
    "    return tabla_final\n",
    "\n",
    "\n",
    "def creando_pasivos_antg(input_paths, date):\n",
    "    \n",
    "    PASIVOS_MONEY_COLS = ['saldo',\n",
    "                          'promedio1',\n",
    "                          'promedio2',\n",
    "                          'promedio3',\n",
    "                          'promedio4',\n",
    "                          'promedio5',\n",
    "                          'promedio6']\n",
    "\n",
    "    dict_cc={0:'Nueva',\n",
    "             1:'Activa',\n",
    "             2:'Bloqueada',\n",
    "             3:'Cancelada',\n",
    "             4:'Inactiva',\n",
    "             5:'Por Cancelar',\n",
    "             6:'Saldada',\n",
    "             7:'Desbloqueada',\n",
    "             8:'Por Saldar',\n",
    "             9:'Por Abrir'}\n",
    "\n",
    "    dict_cdt={1:'Captado',\n",
    "              2:'Depositado',\n",
    "              3:'Exigible',\n",
    "              4:'Cobrado',\n",
    "              5:'Bloquedo',\n",
    "              6:'Embargado',\n",
    "              7:'Retenido',\n",
    "              8:'Anulado',\n",
    "              9:'Cancelado',\n",
    "              10:'Canc. Fraccionamiento',\n",
    "              11:'Canc. Fusion'}\n",
    "\n",
    "    dict_ca={0:'Nueva',\n",
    "             1:'Activa',\n",
    "             2:'Bloqueada',\n",
    "             3:'Cancelada',\n",
    "             4:'Inactiva',\n",
    "             5:'Por Cancelar',\n",
    "             6:'Saldada',\n",
    "             7:'Desbloqueada',\n",
    "             8:'Por Saldar',\n",
    "             9:'Por Abrir'}\n",
    "    \n",
    "    periodo = input_paths['pasivo_cc'][0][-6:]\n",
    "    ## 1. CC\n",
    "    print('Leyendo CC')\n",
    "    input_cc = input_paths['pasivo_cc']+date\n",
    "    cc=spark_read_parquet(input_cc)\n",
    "    cc=cc.rename({'acmcc_scotipide':'tipo_id','id_cli':'identificacion','acmcc_ccu_produ':'cod_producto','acmcc_cfeulttra':'fecha_ultima_transaccion','acmcc_cfeapecue':'fecha_apertura','acmcc_csatotman':'saldo','acmcc_promedmen01':'promedio1','acmcc_promedmen02':'promedio2','acmcc_promedmen03':'promedio3','acmcc_promedmen04':'promedio4','acmcc_promedmen05':'promedio5','acmcc_promedmen06':'promedio6','periodo':'periodo'},axis=1)\n",
    "    cc['nro_cuenta']=cc['cod_producto'].astype('str')+cc['acmcc_ccu_ofici'].astype('str').str.zfill(3)+cc['acmcc_ccunumcue'].astype('str').str.zfill(5)+cc['acmcc_ccudigver'].astype('str') \n",
    "    \n",
    "    cc['estado']=[dict_cc.get(i) for i in cc['acmcc_ccoestcue'] ]\n",
    "    cc=cc[(cc['cod_producto']==110)&(cc['estado'].isin(['Nueva','Activa']))].reset_index().drop(['index'],axis=1)\n",
    "    cc=cc[['tipo_id','identificacion','cod_producto','nro_cuenta','fecha_ultima_transaccion','fecha_apertura','estado','saldo','promedio1','promedio2','promedio3','promedio4','promedio5','promedio6','periodo']]\n",
    "\n",
    "    ## 2. CDT\n",
    "    print('Leyendo CDT')\n",
    "    input_cdt = input_paths['pasivo_cdt']+date\n",
    "    cdt=spark_read_parquet(input_cdt)\n",
    "    cdt=cdt.rename({'afmde_scotipide':'tipo_id','id_cli':'identificacion','afmde_fcu_produ':'cod_producto','afmde_ffeultmod':'fecha_ultima_transaccion','afmde_ffe_depos':'fecha_apertura','afmde_fva_depos':'saldo','afmde_promedio':'promedio1','periodo':'periodo'},axis=1)\n",
    "    cdt['nro_cuenta']=cdt['cod_producto'].astype('str')+cdt['afmde_fcu_ofici'].astype('str').str.zfill(3)+cdt['afmde_fcunumcue'].astype('str').str.zfill(7)+cdt['afmde_fcudigver'].astype('str') \n",
    "    cdt['estado']=[dict_cdt.get(i) for i in cdt['afmde_fco_estde'] ]\n",
    "\n",
    "    cdt['promedio2']=np.nan\n",
    "    cdt['promedio3']=np.nan\n",
    "    cdt['promedio4']=np.nan\n",
    "    cdt['promedio5']=np.nan\n",
    "    cdt['promedio6']=np.nan\n",
    "\n",
    "    cdt=cdt[(cdt['estado'].isin(['Captado','Depositado','Exigible']))].reset_index().drop(['index'],axis=1)\n",
    "    cdt=cdt[['tipo_id','identificacion','cod_producto','nro_cuenta','fecha_ultima_transaccion','fecha_apertura','estado','saldo','promedio1','promedio2','promedio3','promedio4','promedio5','promedio6','periodo']]\n",
    "\n",
    "    ## 3. CA\n",
    "    print('Leyendo CA')\n",
    "    input_ca = input_paths['pasivo_ca']+date\n",
    "    ca=spark_read_parquet(input_ca)\n",
    "    ca=ca.rename({'ahmah_scotipide':'tipo_id','id_cli':'identificacion','ahmah_hcu_produ':'cod_producto','ahmah_hfeulttra':'fecha_ultima_transaccion','ahmah_hfeapecue':'fecha_apertura','ahmah_hsatotman':'saldo','ahmah_hvepromen01':'promedio1','ahmah_hvepromen02':'promedio2','ahmah_hvepromen03':'promedio3','ahmah_hvepromen04':'promedio4','ahmah_hvepromen05':'promedio5','ahmah_hvepromen06':'promedio6','periodo':'periodo'},axis=1)\n",
    "    ca['nro_cuenta']=ca['cod_producto'].astype('str')+ca['ahmah_hcu_ofici'].astype('str').str.zfill(3)+ca['ahmah_hcunumcue'].astype('str').str.zfill(5)+ca['ahmah_hcudigver'].astype('str')  \n",
    "    ca['estado']=[dict_ca.get(i) for i in ca['ahmah_hcoestcue'] ]\n",
    "\n",
    "    ca=ca[(ca['estado'].isin(['Nueva','Activa']))].reset_index().drop(['index'],axis=1)\n",
    "    ca=ca[['tipo_id','identificacion','cod_producto','nro_cuenta','fecha_ultima_transaccion','fecha_apertura','estado','saldo','promedio1','promedio2','promedio3','promedio4','promedio5','promedio6','periodo']]\n",
    "\n",
    "    ## 4. Apppend\n",
    "    print('Append archivos')\n",
    "    total=cc.append(cdt).append(ca).reset_index().drop(['index'],axis=1)\n",
    "    total['fecha'] = date\n",
    "    total['fecha'] = pd.to_datetime(total['fecha'], format='%Y%m')\n",
    "    total['cod_producto']=total['cod_producto'].astype('str')\n",
    "\n",
    "    total['tipo_ident_hom']=np.where(total['tipo_id']==1,'1000003',\n",
    "                                       np.where(total['tipo_id']==2,'1000005',\n",
    "                                               np.where(total['tipo_id']==3,'1000007',\n",
    "                                                       np.where(total['tipo_id']==4,'1000002',\n",
    "                                                               np.where(total['tipo_id']==5,'1000004',\n",
    "                                                                         np.where(total['tipo_id']==6,'1000006',\n",
    "                                                                                   np.where(total['tipo_id']==7,'1000007',\n",
    "                                                                                       np.where(total['tipo_id']==8,'1000008',\n",
    "                                                                                                np.where(total['tipo_id']==9,'1000009','99999')))))))))\n",
    "\n",
    "    frame_total=total\n",
    "    \n",
    "    # Cast\n",
    "    if frame_total['identificacion'].isnull().sum() > 0:\n",
    "        frame_total = frame_total.loc[frame_total['id_cli'].notnull()]\n",
    "\n",
    "    frame_total['identificacion'] = frame_total['identificacion'].astype(np.int64)\n",
    "    frame_total['tipo_ident_hom'] = frame_total['tipo_ident_hom'].astype(np.int64)\n",
    "    \n",
    "    return frame_total\n",
    "\n",
    "\n",
    "def get_pasivos_info(tabla_final):\n",
    "    \"\"\"Deletes duplicates and casts variables\n",
    "    :param activos to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Cast dates\n",
    "    tabla_final['fecha_apertura'] = pd.to_datetime(tabla_final['fecha_apertura'])\n",
    "    tabla_final['fecha_ultima_transaccion'] = pd.to_datetime(tabla_final['fecha_ultima_transaccion'])\n",
    "\n",
    "    # Reset index  ### REVISAR si quitar o no\n",
    "    tabla_final.reset_index(inplace=True)\n",
    "    tabla_final = tabla_final.drop(columns={'index'})\n",
    "\n",
    "    # Cast variables\n",
    "    tabla_final = tabla_final[tabla_final['identificacion'].notnull()]\n",
    "    tabla_final = tabla_final[tabla_final['nro_cuenta'].isna()==False]\n",
    "\n",
    "    tabla_final['identificacion'] = tabla_final['identificacion'].astype('int64')\n",
    "    tabla_final['nro_cuenta'] = tabla_final['nro_cuenta'].astype('int64')\n",
    "    tabla_final['cod_producto'] = tabla_final['cod_producto'].astype('int64')\n",
    "\n",
    "    tabla_final['fecha_apertura'] = pd.to_datetime(tabla_final['fecha_apertura'], format='%Y-%m-%d')\n",
    "    tabla_final['fecha_ultima_transaccion'] = pd.to_datetime(tabla_final['fecha_ultima_transaccion'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Filtro clientes y cast fechas\n",
    "    tabla_final = tabla_final[tabla_final['identificacion'] != -1]\n",
    "    tabla_final['fecha'] = pd.to_datetime(tabla_final['periodo'], format='%Y%m')\n",
    "    \n",
    "    return tabla_final\n",
    "\n",
    "\n",
    "def get_pasivos_last_3(tabla_final):\n",
    "    \"\"\"Takes a product-level data frame and returns a client-level data frame,\n",
    "    :param activos complete activos DataFrame\n",
    "    :return: DataFrame, client-level\n",
    "    \"\"\"\n",
    "\n",
    "    # Validación de duplicados a nivel 'id_cliente','nrooblig','fecha'\n",
    "    pasivos_last_3 = tabla_final.sort_values(['identificacion','nro_cuenta','periodo'])\n",
    "    # Se eliminan duplicados\n",
    "    pasivos_last_3 = pasivos_last_3.drop_duplicates(subset=['identificacion','nro_cuenta','periodo'],keep='first')\n",
    "    \n",
    "    return pasivos_last_3\n",
    "\n",
    "\n",
    "def create_pasivo_last_3_info(pasivos_last_3):\n",
    "    \"\"\"Creates aux variables requeried for the final explanatory variables\n",
    "    :param activos to activos file location\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "       \n",
    "    # Se crea variable\n",
    "    pasivos_last_3['fecha_pasivo'] = pasivos_last_3['periodo'].astype(int)\n",
    "    pasivos_last_3['ano_po'] = pasivos_last_3['periodo'].astype('str').str[0:4].astype(int)\n",
    "\n",
    "    # Se asigna el salario minimo mensual vigente de 2018 y 2019\n",
    "    pasivos_last_3['SMLV'] = np.where(pasivos_last_3['ano_po'] == 2018, 781242,\n",
    "                                            np.where(pasivos_last_3['ano_po'] == 2019, 828116,\n",
    "                                                     np.where(pasivos_last_3['ano_po'] == 2020, 877803,\n",
    "                                                              np.where(pasivos_last_3['ano_po'] == 2021, 908526,\n",
    "                                                              np.nan))))\n",
    "    \n",
    "    # Variable para el delta de fechas\n",
    "    pasivos_last_3['dia_fecha_corte'] = pasivos_last_3['fecha']+timedelta(days=37) ##TODO: Confirmar días\n",
    "\n",
    "    return pasivos_last_3\n",
    "\n",
    "\n",
    "def create_pasivos_df(pasivos_last_3):\n",
    "    \"\"\"Creates client-level final variables\n",
    "    :param activos complete activos DataFrame\n",
    "    :param activos_cli_fec containing activos relevant rows and columns and activos_cli_fec client level data base\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    pasivos_last_3_cli_fec = pd.DataFrame(pasivos_last_3.groupby(['tipo_id', 'identificacion', 'periodo'])[['SMLV','fecha_pasivo']].max())\n",
    "    \n",
    "    ## Agrupaciones\n",
    "    gp_pasv = pasivos_last_3.groupby(['tipo_id','identificacion','periodo'])\n",
    "    gp_pasv_ca = pasivos_last_3[pasivos_last_3['tipo_producto']=='ahorro'].groupby(['tipo_id','identificacion','periodo'])\n",
    "    \n",
    "    ## Antiguedad Total\n",
    "    pasivos_last_3_cli_fec['pasv_antig_total'] = (gp_pasv['dia_fecha_corte'].max() - \n",
    "                                                                    gp_pasv['fecha_apertura'].min())/np.timedelta64(30, 'D')\n",
    "    ## Numero meses ultima apertura\n",
    "    pasivos_last_3_cli_fec['pasv_num_meses_ult_apertura'] = (gp_pasv['dia_fecha_corte'].max() - \n",
    "                                                                    gp_pasv['fecha_apertura'].max())/np.timedelta64(30, 'D')\n",
    "    ## Dia última transacción\n",
    "    pasivos_last_3_cli_fec['pasv_dias_desde_ultima_trans'] = (gp_pasv['dia_fecha_corte'].max() - \n",
    "                                                                    gp_pasv['fecha_ultima_transaccion'].max())/np.timedelta64(1, 'D')\n",
    "    \n",
    "    ## Saldo Total por cartera fin de mes\n",
    "    pasivos_last_3_cli_fec['pasv_saldo_tot_fin_mes_smlv'] = (gp_pasv['saldo'].sum())/(gp_pasv['SMLV'].max())\n",
    "    pasivos_last_3_cli_fec['pasv_saldo_ca_fin_mes_smlv'] = (gp_pasv_ca['saldo'].sum())/(gp_pasv_ca['SMLV'].max())\n",
    "\n",
    "    # Creando fecha\n",
    "    pasivos_last_3_cli_fec.reset_index(inplace=True)\n",
    "    pasivos_last_3_cli_fec['fecha'] = pd.to_datetime(pasivos_last_3_cli_fec['periodo'], format='%Y%m')\n",
    "\n",
    "    return pasivos_last_3_cli_fec\n",
    "\n",
    "\n",
    "def create_pasv_out(pasivos_last_3_cli_fec):\n",
    "    \"\"\"Takes a product-level data frame and returns a client-level data frame,\n",
    "    :param activos complete activos DataFrame\n",
    "    :return: DataFrame, client-level\n",
    "    \"\"\"\n",
    "\n",
    "    str_dates = ['202102','202103','202104'] #### Vector fechas\n",
    "\n",
    "    for ix, date in enumerate(str_dates):\n",
    "        # print(date)\n",
    "        ids = pasivos_last_3_cli_fec.loc[pasivos_last_3_cli_fec['periodo']==int(date), ['tipo_id','identificacion']].drop_duplicates()\n",
    "\n",
    "        if ix == 0:\n",
    "            pasv_out = gp_func(ids, date, pasivos_last_3_cli_fec)\n",
    "        else:    \n",
    "            pasv_temp = gp_func(ids, date, pasivos_last_3_cli_fec)\n",
    "            pasv_out = pd.concat([pasv_out, pasv_temp], ignore_index=True)\n",
    "    \n",
    "    return pasv_out\n",
    "\n",
    "\n",
    "def pasivo_last_3_tot(pasivos_last_3_cli_fec, pasv_out):\n",
    "    \"\"\"Takes a product-level data frame and returns a client-level data frame,\n",
    "    :param activos complete activos DataFrame\n",
    "    :return: DataFrame, client-level\n",
    "    \"\"\"\n",
    "\n",
    "    ## Set_index\n",
    "    pasivos_last_3_cli_fec.set_index(['tipo_id','identificacion','periodo'], inplace=True)\n",
    "    pasv_out.set_index(['tipo_id','identificacion','periodo'], inplace=True)\n",
    "    \n",
    "    # Join de las dos tablas creadas\n",
    "    pasvivos_df = pasivos_last_3_cli_fec.join(pasv_out, how='left')\n",
    "    \n",
    "    # Selección de variables finales\n",
    "    pasvivos_df.reset_index(inplace=True)\n",
    "    pasvivos_df = pasvivos_df.drop(['SMLV','periodo','fecha','pasv_saldo_ca_fin_mes_smlv','pasv_saldo_tot_fin_mes_smlv'], axis=1)\n",
    "    \n",
    "    return pasvivos_df\n",
    "\n",
    "\n",
    "def spark_read_parquet(s3_url: str, **args):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    # Leyendo base\n",
    "    dataset = pq.ParquetDataset(s3_url, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    dataframe = table.to_pandas()\n",
    "\n",
    "    del dataset, table\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def gp_func(ids, date, pasivos_last_3_cli_fec):\n",
    "    fecha = pd.to_datetime(date, format='%Y%m')\n",
    "    n_meses = [1]\n",
    "    \n",
    "    pasv_out = ids.set_index(['tipo_id','identificacion']) # this is different from the actual prod code\n",
    "    \n",
    "    for n in n_meses:\n",
    "        col_name = 'pasv_saldo_total_'+str(n)+'mes_atras'\n",
    "        col_name_ca = 'pasv_saldo_ca_'+str(n)+'mes_atras'\n",
    "        date_past = fecha - timedelta(days=n*30)\n",
    "\n",
    "        gp = pasivos_last_3_cli_fec[(pasivos_last_3_cli_fec['fecha']>date_past)&(pasivos_last_3_cli_fec['fecha']<=fecha)].groupby(['tipo_id','identificacion'])\n",
    "        \n",
    "        pasv_out[col_name] = gp['pasv_saldo_tot_fin_mes_smlv'].sum()/n\n",
    "        pasv_out[col_name_ca] = gp['pasv_saldo_ca_fin_mes_smlv'].sum()/n\n",
    "        \n",
    "    pasv_out['periodo'] = int(date)\n",
    "    #pasv_out.index.rename('id_cliente', inplace=True)\n",
    "    pasv_out.reset_index(inplace=True)\n",
    "    \n",
    "    return pasv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasvivos_df = process_pasivos(input_paths_activos)\n",
    "# print(pasvivos_df)\n",
    "# pasvivos_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7bae8076a577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'aaaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Consolidado Premdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidado_premdt(ids_final2):\n",
    "\n",
    "    poblacion_total = ids_final2.drop_duplicates(subset=['id_numero_cliente','fecha_camp'], keep='first')\n",
    "    poblacion_total['periodo'] = '07-2021'\n",
    "    poblacion_total['periodo'].value_counts().sort_index()\n",
    "    \n",
    "    #Creación de fecha correspondiente para cruzar con la base campañas\n",
    "    poblacion_total['data_camp'] = np.where((poblacion_total['periodo'] == '07-2021'), 202105, 199901)\n",
    "    #Creación de fecha correspondiente para cruzar con la base buró\n",
    "    poblacion_total['fecha_buro'] = np.where((poblacion_total['periodo'] == '07-2021'), 202101, 199901)\n",
    "    #Creación de fecha correspondiente para cruzar con la base activo\n",
    "    poblacion_total['fecha_activos'] = np.where((poblacion_total['periodo'] == '07-2021'), 202105, 199901)\n",
    "    #Creación de fecha correspondiente para cruzar con la base pasivo\n",
    "    poblacion_total['fecha_pasivo'] = np.where((poblacion_total['periodo'] == '07-2021'), 202105, 199901)\n",
    "    #Creación de fecha correspondiente para cruzar con la base libranza\n",
    "    poblacion_total['fecha_lib'] = np.where((poblacion_total['periodo'] == '07-2021'), 202106, 199901)\n",
    "    #Creación de fecha correspondiente para cruzar con la base tx pasivo\n",
    "    poblacion_total['fecha_tx_pav'] = np.where((poblacion_total['periodo'] == '07-2021'), 202105, 199901)\n",
    "    \n",
    "    # Cast ID\n",
    "    poblacion_total = poblacion_total[poblacion_total['id_numero_cliente'].notnull()]\n",
    "    poblacion_total['id_cliente'] = poblacion_total['id_cliente'].astype(int)\n",
    "    poblacion_total['id_numero_cliente'] = poblacion_total['id_numero_cliente'].astype(int)\n",
    "    poblacion_total = poblacion_total.drop_duplicates()\n",
    "    \n",
    "    return poblacion_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poblacion_total = consolidado_premdt(ids_final2)\n",
    "print(poblacion_total.shape)\n",
    "poblacion_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = { 'libranzas': ['s3://data-bpop-dev-sandbox/estandarizado/productos/libranzas/productos_libranzas_dwh_M202104'],  ## Mensual 1 mes atras respecto al mes que esta corriendo - coger v2\n",
    "         'clientes': ['s3://data-bpop-dev-sandbox/estandarizado/clientes/identificacion/clientes_identificacion_mdm_D20210218'],  ## Mensual 1 mes atras - coger v2 #### NO SE NECESITA QUITAR\n",
    "\n",
    "         'activo': ['s3://data-bpop-dev-sandbox/estandarizado/productos/activo/productos_activo_sarc_M202104'],  ## Mensual 2 meses atras - coger v2\n",
    "\n",
    "         'canpañas': 's3://data-bpop-dev-refined/gestion-campanas/preaprobado-tc/',\n",
    "         'crm': ['s3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20200228-20200601.csv',  ## Quemado\n",
    "                 's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20190801-20200201.csv',  ## Quemado\n",
    "                 's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/campanas-tc-nuevas-proc-crm_H20190104-20190702.csv'],  ## Quemado\n",
    "         'vendors':['s3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-vend/campanas-tc-nuevas-proc-vendors_H20200401-20200601.csv',  ## Quemado\n",
    "                    's3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-vend/campanas-tc-nuevas-proc-vendors_H20190101-20200301.csv'],  ## Quemado\n",
    "\n",
    "         'activaciones': ['s3://adl-refined-dev-popular/data_orig/productos/activo-tarjeta-credito-activadas/productos_activos-tarjeta-credito-activadas_H202003-202005.csv',  ## Quemado\n",
    "                          's3://adl-refined-dev-popular/data_orig/productos/activo-tarjeta-credito-activadas/productos_activos-tarjeta-credito-activadas_H20180101-20200327.csv'],  ## Quemado\n",
    "         'acti_diarias': ['s3://data-bpop-dev-sandbox/estandarizado/productos/activo-tarjeta-credito-nueva'],  ##TodosDiarios\n",
    "\n",
    "         'buro': 's3://data-bpop-dev-sandbox/estandarizado/riesgos-creditos/buro-comportamiento-financiero',  ##TodosDiarios\n",
    "\n",
    "         'pasivo_cc': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-cc/productos_pasivo-sfb-cc_aaatr_M',\n",
    "\n",
    "         'pasivo_cdt': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-cdts/productos_pasivo-sfb-cdts_aaatr_M',\n",
    "                       \n",
    "         'pasivo_ca': 's3://data-bpop-dev-sandbox/estandarizado/productos/pasivo-sfb-ca/productos_pasivo-sfb-ca_aaatr_M'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = get_s3_parquet_flag(input_paths['canpañas'], 0)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = input_paths['canpañas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths['canpañas'] + list_files(input_paths['canpañas'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key_files__list(client, bucket, key):\n",
    "    \"\"\"return the key's size if it exist, else None\"\"\"\n",
    "    response = client.list_objects_v2(Bucket=bucket, Prefix=key)\n",
    "    return response.get(\"Contents\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_parquets(s3_url):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client(\"s3\")\n",
    "    bucket = s3_url.split(\"/\")[2]\n",
    "    key = s3_url.split(\"/\")[3:]\n",
    "    key = \"/\".join(key)\n",
    "    objects = _key_files__list(s3, bucket, key)\n",
    "    files = [\"s3://\" + bucket + \"/\" + file[\"Key\"][:-9] for file in objects if file[\"Key\"].split(\"/\")[-1]==\"_SUCCESS\"]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\"s3\")\n",
    "bucket = s3_url.split(\"/\")[2]\n",
    "key = s3_url.split(\"/\")[3:]\n",
    "key = \"/\".join(key)\n",
    "objects = _key_files__list(s3, bucket, key)\n",
    "files = [\"s3://\" + bucket + \"/\" + file[\"Key\"] for file in objects]# if file[\"Key\"].split(\"/\")[-1]==\"_SUCCESS\"]\n",
    "#return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_url = input_paths['canpañas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##5. ACTIVACIONES\n",
    "def get_activaciones_file(input_paths):\n",
    "\n",
    "    data_act2 = pd.read_csv(input_paths['activaciones'][0], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act2 = data_act2.drop_duplicates()\n",
    "    print(data_act2.shape)\n",
    "    \n",
    "    data_act1 = pd.read_csv(input_paths['activaciones'][1], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act1 = data_act1.drop_duplicates()\n",
    "    print(data_act1.shape)\n",
    "    \n",
    "    # Uniendo bases\n",
    "    data_act2['fecha_activacion'] = data_act2['fecha_activacion'].str.replace('/20','/2020')\n",
    "    data_act1 = pd.concat([data_act2,data_act1], ignore_index=True)\n",
    "    \n",
    "    # Casting variables\n",
    "    data_act1['fecha_activacion'] = pd.to_datetime(data_act1['fecha_activacion'],format='%d/%m/%Y')\n",
    "    data_act1['mes_activacion'] = pd.to_datetime(data_act1['fecha_activacion'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_act1.columns = data_act1.columns.str.lower()\n",
    "    data_act1 = data_act1.rename(columns={'identificacion':'cedula'})\n",
    "    del data_act2\n",
    "    \n",
    "    return data_act1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_act2 = pd.read_csv(input_paths['activaciones'][0], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "data_act2 = data_act2.drop_duplicates()\n",
    "print(data_act2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths['activaciones'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files('s3://adl-refined-dev-popular/data_orig/campanas/tc-nuevas-proc-crm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import boto3\n",
    "import boto3.session\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def process_camp_activos(input_paths_activos):\n",
    "    \"\"\"Pending docstring\"\"\"\n",
    "    \n",
    "    # Pre_crm. from different route\n",
    "    data_crm1 = get_crm_new_file(input_paths_activos)\n",
    "    print('get_crm_new_file')\n",
    "    # Pre_vend. from different route\n",
    "    data_vend1 = get_vend_new_file(input_paths_activos)\n",
    "    print('get_vend_new_file')\n",
    "    \n",
    "    # 1. Get crm file\n",
    "    data_crm = get_crm_file(input_paths_activos)\n",
    "    print('get_crm_file')\n",
    "    # 2. Deletes duplicates and casts variables\n",
    "    data_crm = get_crm_info(data_crm, data_crm1)\n",
    "    print('get_crm_info')\n",
    "    print(data_crm.head())\n",
    "    \n",
    "    # 3. Get vend file\n",
    "    data_vend = get_vend_file(input_paths_activos)\n",
    "    print('get_vend_file')\n",
    "    # 4. Deletes duplicates, casts variables and union data_crm - data_vend\n",
    "    data_camp = get_vend_info(data_vend, data_crm, data_vend1)\n",
    "    print('get_vend_info')\n",
    "    print(data_camp.head())\n",
    "    \n",
    "    # 5. Get activaciones file\n",
    "    data_act1 = get_activaciones_file(input_paths_activos)\n",
    "    print('get_activaciones_file')\n",
    "    # 6. Get activaciones_diarias file\n",
    "    data_act = get_acti_diarias_file(input_paths_activos)\n",
    "    print('get_acti_diarias_file')\n",
    "    # 7. Deletes duplicates and casts variables\n",
    "    data_act = get_act_diarias_info(data_act, data_act1)\n",
    "    print('get_act_diarias_info')\n",
    "    \n",
    "    # 8. Creates aux variables requeried for the final explanatory variables\n",
    "    base_final = create_func_activa(data_camp, data_act)\n",
    "    print('create_func_activa')\n",
    "#    # 9. Deletes duplicates and casts variables\n",
    "#    base_final = get_func_activa_info(base_final)\n",
    "#    print('get_func_activa_info')\n",
    "#    \n",
    "#    # 10. Creates aux variables requeried for the final explanatory variables\n",
    "#    tabla_final = create_func_var_camp_act(base_final)\n",
    "#    print('create_func_var_camp_act')\n",
    "#    # 11. Deletes duplicates and casts variables\n",
    "#    tabla_final = get_func_var_camp_act(tabla_final)\n",
    "#    print('get_func_var_camp_act')\n",
    "#    print('camp_activos_df created successfully')  \n",
    "    \n",
    "    return base_final\n",
    "\n",
    "#########################################################\n",
    "################### Bases de campañas ###################\n",
    "#########################################################\n",
    "\n",
    "#####Prep. crm_new\n",
    "def get_crm_new_file(input_paths):\n",
    "    \n",
    "    path_file = get_s3_parquet_flag(input_paths['canpañas'], 0)[-1]\n",
    "    data_temp = pd.read_csv(path_file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "    data_temp = data_temp.sort_values(['base_name','date','clientid'])\n",
    "    data_temp = data_temp.groupby(['clientid','base_name']).first()\n",
    "    data_temp.reset_index(inplace=True)\n",
    "    \n",
    "    ## Filtrar express-la14\n",
    "    data_temp = data_temp.loc[~data_temp['base_name'].str.contains('EXPRES_LA14')]\n",
    "    data_temp = data_temp.rename(columns={'clientid':'NUM_DOC'})\n",
    "    \n",
    "    ## Oficinas\n",
    "    data_crm = data_temp.loc[~data_temp['base_name'].str.contains('OFICINA_BOGOTA')]\n",
    "    ## tipo campaña\n",
    "    data_crm['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_crm['tipo_camp'] = 'of-preaprobados'\n",
    "    # Casting variables\n",
    "    data_crm['FECHA'] = pd.to_datetime(data_crm['date'],format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(np.where(data_crm['FECHA'].dt.day>=28, \n",
    "                                                      data_crm['FECHA']+timedelta(days=10),\n",
    "                                                      data_crm['FECHA']),format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña2'] = pd.to_datetime(data_crm['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(data_crm['mes_campaña'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm1 = data_crm.copy()\n",
    "    \n",
    "    return data_crm1\n",
    "\n",
    "\n",
    "#####Prep. vend_new\n",
    "def get_vend_new_file(input_paths):\n",
    "    \n",
    "    path_file = get_s3_parquet_flag(input_paths['canpañas'], 0)[-1]\n",
    "    data_temp = pd.read_csv(path_file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "    data_temp = data_temp.sort_values(['base_name','date','clientid'])\n",
    "    data_temp = data_temp.groupby(['clientid','base_name']).first()\n",
    "    data_temp.reset_index(inplace=True)\n",
    "    \n",
    "    ## Filtrar express-la14\n",
    "    data_temp = data_temp.loc[~data_temp['base_name'].str.contains('EXPRES_LA14')]\n",
    "    data_temp = data_temp.rename(columns={'clientid':'NUM_DOC'})\n",
    "    \n",
    "    data_vend = data_temp.loc[~data_temp['base_name'].str.contains('CALL_ATENTO')]\n",
    "    # Casting variables\n",
    "    data_vend['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_vend['tipo_camp'] = 'at-preaprobados'\n",
    "    data_vend['FECHA'] = pd.to_datetime(data_vend['date'],format='%Y-%m-%d')\n",
    "    data_vend['mes_campaña'] = pd.to_datetime(data_vend['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_vend1 = data_vend.copy()\n",
    "    \n",
    "    return data_vend1\n",
    "\n",
    "\n",
    "#####1. OFICINAS\n",
    "\n",
    "def get_crm_file(input_paths):\n",
    "    files = input_paths['crm']\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        \n",
    "        data_temp = pd.read_csv(file, sep='|', dtype={'ID_CLIENTE':str,'NUM_DOC':str}) \n",
    "        \n",
    "        if ix == 0:\n",
    "            data_crm = data_temp\n",
    "        else:\n",
    "            data_crm = pd.concat([data_crm,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "    \n",
    "    return data_crm\n",
    "\n",
    "## 2.    \n",
    "## tipo campaña\n",
    "def get_crm_info(data_crm, data_crm1):\n",
    "    \n",
    "    data_crm['TIPO CAMPAÑA'] = 'Preaprobados'\n",
    "    data_crm['tipo_camp'] = 'of-preaprobados'\n",
    "    \n",
    "    # Casting variables\n",
    "    data_crm['FECHA'] = pd.to_datetime(data_crm['FECHA'],format='%d/%m/%Y')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(np.where(data_crm['FECHA'].dt.day>=28,\n",
    "                                                      data_crm['FECHA']+timedelta(days=10), \n",
    "                                                      data_crm['FECHA']),format='%Y-%m-%d')\n",
    "    data_crm['mes_campaña2'] = pd.to_datetime(data_crm['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_crm['mes_campaña'] = pd.to_datetime(data_crm['mes_campaña'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    data_crm = pd.concat([data_crm1, data_crm], ignore_index=True)\n",
    "    \n",
    "    return data_crm\n",
    "\n",
    "\n",
    "##### 3. ATENTO\n",
    "def get_vend_file(input_paths):\n",
    "\n",
    "    files = input_paths['vendors']\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        \n",
    "        data_temp = pd.read_csv(file, sep='|', encoding='ISO8859-1',quoting=1, dtype={'ID_CLIENTE':str,'NUM_DOC':str})\n",
    "        if ix == 1:\n",
    "            data_temp['FECHA2'] = pd.to_datetime(data_temp['FECHA'],format='%d/%m/%Y')\n",
    "            data_temp = data_temp[data_temp['FECHA2']<'2020-12-01']\n",
    "            data_temp = data_temp[['TIP_DOC','NUM_DOC','ID_CLIENTE',' CUPO_APROBADO ','FECHA','TIPO CAMPAÑA']]\n",
    "        if ix == 0:\n",
    "            data_vend = data_temp\n",
    "        else:\n",
    "            data_vend = pd.concat([data_vend,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "    \n",
    "    return data_vend\n",
    "\n",
    "\n",
    "#4. Casting variables\n",
    "def get_vend_info(data_vend, data_crm, data_vend1):\n",
    "    \n",
    "    data_vend['tipo_camp'] = np.where(data_vend['TIPO CAMPAÑA'] == 'Preaprobados','at-preaprobados','at-perfilados')\n",
    "    data_vend['FECHA'] = pd.to_datetime(data_vend['FECHA'],format='%d/%m/%Y')\n",
    "    data_vend['mes_campaña'] = pd.to_datetime(data_vend['FECHA'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    data_vend = pd.concat([data_vend1,data_vend], ignore_index=True)\n",
    "    \n",
    "    cols = ['NUM_DOC', 'FECHA', 'mes_campaña', 'TIPO CAMPAÑA', 'tipo_camp']\n",
    "    \n",
    "    data_vend.columns = data_vend.columns.str.strip()\n",
    "    data_camp = pd.concat([data_crm[cols], data_vend[cols]], ignore_index=True)[cols]\n",
    "        \n",
    "    data_camp['NUM_DOC'] = data_camp['NUM_DOC'].astype(np.int64)\n",
    "    data_camp['FECHA'] = pd.to_datetime(data_camp['FECHA'])\n",
    "    data_camp['mes_campaña'] = pd.to_datetime(data_camp['mes_campaña'])\n",
    "    data_camp = data_camp.rename(columns={'NUM_DOC':'cedula'})\n",
    "    \n",
    "    return data_camp\n",
    "\n",
    "    \n",
    "##5. ACTIVACIONES\n",
    "def get_activaciones_file(input_paths):\n",
    "\n",
    "    data_act2 = pd.read_csv(input_paths['activaciones'][0], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act2 = data_act2.drop_duplicates()\n",
    "    print(data_act2.shape)\n",
    "    \n",
    "    data_act1 = pd.read_csv(input_paths['activaciones'][1], sep = '|', dtype={'ID_CLIENTE':str,'Numero_cuenta':str,'Identificacion':str})\n",
    "    data_act1 = data_act1.drop_duplicates()\n",
    "    print(data_act1.shape)\n",
    "    \n",
    "    # Uniendo bases\n",
    "    data_act2['fecha_activacion'] = data_act2['fecha_activacion'].str.replace('/20','/2020')\n",
    "    data_act1 = pd.concat([data_act2,data_act1], ignore_index=True)\n",
    "    \n",
    "    # Casting variables\n",
    "    data_act1['fecha_activacion'] = pd.to_datetime(data_act1['fecha_activacion'],format='%d/%m/%Y')\n",
    "    data_act1['mes_activacion'] = pd.to_datetime(data_act1['fecha_activacion'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    data_act1.columns = data_act1.columns.str.lower()\n",
    "    data_act1 = data_act1.rename(columns={'identificacion':'cedula'})\n",
    "    del data_act2\n",
    "    \n",
    "    return data_act1\n",
    "\n",
    "\n",
    "##6. bases diarias\n",
    "def get_acti_diarias_file(input_paths):\n",
    "\n",
    "    files = get_s3_parquets(input_paths['acti_diarias'][0])\n",
    "    \n",
    "    for ix, file in enumerate(files):\n",
    "        print('File:', file)\n",
    "        data_temp = spark_read_parquet(file)\n",
    "        if ix == 0:\n",
    "            data_act = data_temp\n",
    "        else:\n",
    "            data_act = pd.concat([data_act,data_temp], ignore_index=True)\n",
    "            \n",
    "        del data_temp\n",
    "        \n",
    "    return data_act\n",
    "\n",
    "        \n",
    "#7. Casting variables     \n",
    "def get_act_diarias_info(data_act, data_act1):\n",
    "\n",
    "    data_act['fecha_activacion_plastico'] = pd.to_datetime(data_act['fecha_activacion_plastico'])\n",
    "    data_act['mes_activacion'] = pd.to_datetime(data_act['fecha_activacion_plastico'].dt.strftime('%Y-%m'),format='%Y-%m')\n",
    "    \n",
    "    ## Renombrando variables\n",
    "    data_act = data_act.rename(columns={'id_cli':'cedula',\n",
    "                                       'tipo_id_cliente':'tipo_identificacion',\n",
    "                                       'fecha_activacion_plastico':'fecha_activacion'})\n",
    "    \n",
    "    data_act = data_act[['numero_cuenta', 'numero_tarjeta', 'cedula', 'tipo_identificacion', 'fecha_activacion', 'id_cliente',\n",
    "           'mes_activacion']]\n",
    "    \n",
    "    ## Uniendo bases\n",
    "    data_act = pd.concat([data_act1, data_act], ignore_index=True)\n",
    "    data_act = data_act.drop_duplicates()\n",
    "\n",
    "    del data_act1\n",
    "\n",
    "    ## Cast variables\n",
    "    data_act['id_cliente'] = data_act['id_cliente'].fillna('-9999').astype(np.int64)\n",
    "    data_act['tipo_identificacion'] = data_act['tipo_identificacion'].fillna('-9999').astype(np.int64)\n",
    "    data_act['cedula'] = data_act['cedula'].astype(np.int64)\n",
    "\n",
    "    return data_act\n",
    "\n",
    "\n",
    "###8. func_activa\n",
    "def create_func_activa(data_camp, data_act):\n",
    "\n",
    "    str_dates = ['2019-01','2019-02','2019-03','2019-04','2019-05','2019-06','2019-07','2019-08','2019-09','2019-10','2019-11',\n",
    "                 '2019-12','2020-01','2020-02','2020-03','2020-04','2020-05','2020-06','2020-07','2020-08','2020-09','2020-10', \n",
    "                 '2020-11','2020-12','2021-01','2021-02','2021-03','2021-04','2021-05','2021-06']\n",
    "    n_mes = 3 # numero de meses\n",
    "    \n",
    "    for ix, date in enumerate(str_dates):\n",
    "        print('Fecha:',date)\n",
    "        date = pd.to_datetime(date,format='%Y-%m')\n",
    "        nom_col = 'camp_'+date.strftime('%Y%m')\n",
    "        \n",
    "        base = data_camp[data_camp['mes_campaña']==date]\n",
    "    \n",
    "        if ix == 0:\n",
    "            base_final = func_activa(base, data_act, date, n_mes, nom_col)\n",
    "            del base\n",
    "        else:\n",
    "            base_temp = func_activa(base, data_act, date, n_mes, nom_col)\n",
    "            base_final = pd.concat([base_final, base_temp], ignore_index=True)\n",
    "            \n",
    "            del base, base_temp\n",
    "\n",
    "    return base_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_final = process_camp_activos(input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
